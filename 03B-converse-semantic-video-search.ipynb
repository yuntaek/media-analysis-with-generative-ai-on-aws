{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8458ac4-23ed-461e-bf44-55432f709d1a",
   "metadata": {},
   "source": [
    "# Semantic Video Search\n",
    "\n",
    "Semantic video search is an advanced technology that enables users to find and retrieve video content based on its meaning and context, rather than relying solely on keywords or metadata. Content creators produce videos with rich, multidimensional information. The semantic search algorithm analyzes and understands this content using AI technologies like computer vision, natural language processing, and now, generative AI. This allows users to interact with the system, searching for specific concepts, objects, or actions within videos.\n",
    "\n",
    "![Semantic Video Search](./static/images/04-semantic-video-search.png)\n",
    "\n",
    "Semantic video search is crucial for the media and entertainment industry as it dramatically improves content discovery, user engagement, and the overall viewing experience. In an era of content overload, users demand more efficient ways to find relevant videos. Traditional search methods often fall short, leading to frustrated users and underutilized content libraries. Semantic search allows media companies to unlock the full potential of their video archives, improve recommendation systems, and create more personalized viewing experiences.\n",
    "\n",
    "However, implementing effective semantic video search comes with significant challenges. The sheer volume and complexity of video data make it difficult to analyze and index content accurately. Variations in video quality, language, and cultural context can lead to misinterpretations. Generative AI offers a promising solution to enhance semantic video search capabilities. By leveraging large language models and multimodal AI, generative AI can provide more nuanced and context-aware analysis of video content. It can generate detailed descriptions of scenes, identify complex actions and emotions, and even understand subtle cultural references, bridging the gap between user intent and video content.\n",
    "\n",
    "In this lab, you'll create a multi-modal vector database using visual and audio metadata generated from previously labs to build a multi-modal(MM) search database. By the end of the lab, you'll be able to query against this database using natural language or images, and find quickly find the relevant shots from the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743be170-1b38-4ba1-b33b-f19841acd135",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0292f5c5-22ed-4d89-906c-a9424ccdf01f",
   "metadata": {},
   "source": [
    "To run this notebook, you need to have run all the previous foundation notebooks, where you set up the notebook environment and segmented the video using audio, visual and semantic information:\n",
    "1. [00-prerequisites.ipynb](00-prerequisites.ipyn)\n",
    "2. [01A-visual-segments-frames-shots-scenes.ipynb](01A-visual-segments-frames-shots-scenes.ipynb) \n",
    "3. [01B-audio-segments.ipynb](01B-audio-segments.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870de375-fcf0-46f4-9154-1b54fb7b7737",
   "metadata": {},
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c286d5-2bf1-4aff-99ca-74b9d929c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from IPython.display import display, JSON, HTML\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "import base64\n",
    "from termcolor import colored\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324a9e1-fbc5-47d3-81f1-24528f9dfba7",
   "metadata": {},
   "source": [
    "### Retrieve saved values from previous notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13068f49-0a48-4d1a-ac1c-c504dc0a58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f749e7-42c4-4c40-8079-6e180e32c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = video['path']\n",
    "rek_client = boto3.client(\"rekognition\")\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "region = sagemaker_resources['region']\n",
    "oss_host = \"https://v8u51et003r4u04ep9k9.us-west-2.aoss.amazonaws.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8ae5c-a196-4906-8c85-66ca6002ec64",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "This hands-on workflow uses AWS services executed from SageMaker.  It sends shot level segmentation information generated from lab01 (shot groups, audio transcription, shot level composite images) to various AWS AI/GenAI services to generate embeddings and metadata to power our semantic video search solution. You will complete the following steps:\n",
    "\n",
    "1. You first send shot level frames to Amazon Rekognition for celebrity detection.\n",
    "2. Then the celebrity information and shot level composite image are sent to Anthropic Claude 3 Sonnet model in Bedrock to generate shot level caption. The caption is then converted to text embeddings using Amazon Titan Text Embedding model, also from Bedrock.\n",
    "3. Shot level composite image is also sent to Amazon Titan Multi-model(MM) Embedding model to generate image search embeddings.\n",
    "\n",
    "You will index the embeddings and metadata in OpenSearch Serverless as your vector database. Once data ingestion is complete, you can search against that database using both text and image to find the closest match shots from the video.\n",
    "\n",
    "![Flow diagram](./static/images/04-lab-flow-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98928c85-38f7-49f1-b548-0d15d01f19ae",
   "metadata": {},
   "source": [
    "## Randomly Sample a subset of shots from the video\n",
    "\n",
    "For a better and uninterrupted lab experience, we will randomly sample 10 shots from the original videos in chronical order. This approach is necessary due to the limited capacity in the workshop environment. Do you so will still maintain the integrity of the exercise while ensuring that all participants can complete the lab without getting throttled. If you are not in the workshop sandbox environment, please feel free to increase the number of shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8d2ea-8858-4c24-87a6-c9c108826186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shot_mapping = {\n",
    "    \"Netflix_Open_Content_Meridian.mp4\": [1, 8, 12, 14, 21, 36, 46, 53, 61, 66],\n",
    "}\n",
    "\n",
    "# select the shot mapping\n",
    "assert video['path'] in shot_mapping, f\"****[{video['path']}]*** is not a supported video.\"\n",
    "\n",
    "# Assert that the key is in the dictionary\n",
    "shot_ids = shot_mapping[video['path']]\n",
    "\n",
    "sampled_shots = []\n",
    "\n",
    "for shot in video['shots'].shots:\n",
    "\n",
    "    if shot['id'] in shot_ids:\n",
    "        sampled_shots.append(shot)\n",
    "        print(colored(f\"Sampled shot id: {shot['id']} ===================\\n\", \"green\"))\n",
    "        display(Image.open(shot['composite_images'][0]['file']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351d55a-0263-4500-9d0a-f9314fbcb4c2",
   "metadata": {},
   "source": [
    "## Detect Celebrities Using Amazon Rekognition\n",
    "[Amazon Rekognition](https://aws.amazon.com/rekognition/) can be used to recognize international, widely known celebrities like actors, sportspeople, and online content creators. The metadata provided by the celebrity recognition API significantly reduces the repetitive manual effort required to tag content and make it readily searchable. In the following section, we'll leverage this feature to help us detect any celebrities in the shots extracted in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400dce05-ad3a-4751-9f58-4d3ac885bcc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detect_celebrities(shot):\n",
    "    start_frame_id = shot['start_frame_id']\n",
    "    end_frame_id = shot['end_frame_id']\n",
    "    video_asset_dir = shot['video_asset_dir']\n",
    "\n",
    "    frames = range(start_frame_id, end_frame_id + 1)\n",
    "\n",
    "    celebrities = set()\n",
    "\n",
    "    for frame_id in frames:\n",
    "        try:\n",
    "            #image_path = f\"{video_asset_dir}/frames/frames{frame_id+1:07d}.jpg\"\n",
    "            image_path = f\"{video_asset_dir}/frames/frames{frame_id+1:07d}.jpg\"\n",
    "            with open(image_path, 'rb') as image_file:\n",
    "                image_bytes = image_file.read()      \n",
    "\n",
    "            # Call Rekognition to detect celebrities\n",
    "            response = rek_client.recognize_celebrities(\n",
    "                Image={'Bytes': image_bytes}\n",
    "            )\n",
    "\n",
    "            min_confidence = 95.0 # change this value if the accuracy is low.\n",
    "\n",
    "            for celebrity in response.get('CelebrityFaces', []):\n",
    "                if celebrity.get('MatchConfidence', 0.0) >= min_confidence:\n",
    "                    celebrities.add(celebrity['Name'])\n",
    "\n",
    "        except ClientError as e:\n",
    "            pass\n",
    "\n",
    "    public_figures = ', '.join(celebrities)\n",
    "\n",
    "    shot[\"public_figure\"] = public_figures\n",
    "    \n",
    "    return {\n",
    "            \"shot_id\": shot['id'],\n",
    "            \"public_figure\": public_figures\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5207574-8777-4170-94fc-3792dc59b130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(colored(\"===== [Celebrities detected in each shot] ======\\n\", 'green'))\n",
    "for shot in sampled_shots:\n",
    "    print(detect_celebrities(shot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3257c5c-eb42-4950-8470-21c47a68b42e",
   "metadata": {},
   "source": [
    "## Process Audio Transcription\n",
    "\n",
    "Convert subtitles to sentences with timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bcbf6e-a367-495c-8c40-858e61c81257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcript(s):\n",
    "    subtitle_blocks = re.findall(\n",
    "        r\"(\\d+\\n(\\d{2}:\\d{2}:\\d{2}.\\d{3}) --> (\\d{2}:\\d{2}:\\d{2}.\\d{3})\\n(.*?)(?=\\n\\d+\\n|\\Z))\",\n",
    "        s,\n",
    "        re.DOTALL,\n",
    "    )\n",
    "\n",
    "    sentences = [block[3].replace(\"\\n\", \" \").strip() for block in subtitle_blocks]\n",
    "    startTimes = [block[1] for block in subtitle_blocks]\n",
    "    endTimes = [block[2] for block in subtitle_blocks]\n",
    "\n",
    "    startTimes_ms = [time_to_ms(time) for time in startTimes]\n",
    "    endTimes_ms = [time_to_ms(time) for time in endTimes]\n",
    "\n",
    "    filtered_sentences = []\n",
    "    filtered_startTimes_ms = []\n",
    "    filtered_endTimes_ms = []\n",
    "\n",
    "    startTime_ms = -1\n",
    "    endTime_ms = -1\n",
    "    sentence = \"\"\n",
    "    for i in range(len(sentences)):\n",
    "        if startTime_ms == -1:\n",
    "            startTime_ms = startTimes_ms[i]\n",
    "        sentence += \" \" + sentences[i]\n",
    "        if (\n",
    "            sentences[i].endswith(\".\")\n",
    "            or sentences[i].endswith(\"?\")\n",
    "            or sentences[i].endswith(\"!\")\n",
    "            or i == len(sentences) - 1\n",
    "        ):\n",
    "            endTime_ms = endTimes_ms[i]\n",
    "            filtered_sentences.append(sentence.strip())\n",
    "            filtered_startTimes_ms.append(startTime_ms)\n",
    "            filtered_endTimes_ms.append(endTime_ms)\n",
    "            startTime_ms = -1\n",
    "            endTime_ms = -1\n",
    "            sentence = \"\"\n",
    "\n",
    "    processed_transcript = []\n",
    "    for i in range(len(filtered_sentences)):\n",
    "        processed_transcript.append(\n",
    "            {\n",
    "                \"sentence_startTime\": filtered_startTimes_ms[i],\n",
    "                \"sentence_endTime\": filtered_endTimes_ms[i],\n",
    "                \"sentence\": filtered_sentences[i],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return processed_transcript\n",
    "\n",
    "def time_to_ms(time_str):\n",
    "    h, m, s, ms = re.split(r\"[:|.]\", time_str)\n",
    "    return int(h) * 3600000 + int(m) * 60000 + int(s) * 1000 + int(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339e88a-4e83-4e1d-a3ad-486350f7de05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(video['transcript'].vtt_file, encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "processed_transcript = process_transcript(transcript)\n",
    "\n",
    "print(colored(\"===== [Complete Sentences W/ Timestamps] ======\\n\", \"green\"))\n",
    "processed_transcript[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b0a6d-24f8-41f9-ae5a-de529fe02981",
   "metadata": {},
   "source": [
    "## Align sentences to shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080549d-e492-45a4-801c-5a311e00eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shot_transcript(shot_startTime, shot_endTime, transcript):\n",
    "    relevant_transcript = \"\"\n",
    "    for item in transcript:\n",
    "        if item[\"sentence_startTime\"] >= shot_endTime:\n",
    "            break\n",
    "        if item[\"sentence_endTime\"] <= shot_startTime:\n",
    "            continue\n",
    "        delta_start = max(item[\"sentence_startTime\"], shot_startTime)\n",
    "        delta_end = min(item[\"sentence_endTime\"], shot_endTime)\n",
    "        if delta_end - delta_start >= 500:\n",
    "            relevant_transcript += item[\"sentence\"] + \"; \"\n",
    "    return relevant_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34349e7c-1135-4956-8819-74d60dcb5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"===== [Complete sentence align to every shot] ======\\n\", \"green\"))\n",
    "\n",
    "for shot in sampled_shots:\n",
    "    relevant_transcript = add_shot_transcript(shot['start_ms'], shot['end_ms'], processed_transcript)\n",
    "    shot['transcript'] = relevant_transcript\n",
    "    print({\n",
    "        'shot_id': shot['id'],\n",
    "        'transcript': relevant_transcript\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d60506-0258-4a62-9c44-8b88e79d1d18",
   "metadata": {},
   "source": [
    "## Create the Shot Description \n",
    "Leverage an LLM to extract key elements from the frame images that belong to a shot.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "⏳ This step leverages Anthropic Claude Sonnet 3 model to extract key elements from the composite frame images, and it may take more than 2 minutes to run.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e8b88-2b67-4ce8-ac69-6b9dfe2f30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shot_description(model_id, composite_images, celebrities):\n",
    "\n",
    "    system_prompts = [{\"text\": \"\"\"\n",
    "    You are an expert video content analyst specializing in generating rich, contextual metadata for semantic search systems. \n",
    "    Your task is to analyze video shots presented in a sequence of frame images and provide a detailed but concise description \n",
    "    of a video shot based on the given frame images. Focus on creating a cohesive narrative of the entire shot rather than \n",
    "    describing each frame individually.\n",
    "    \"\"\"}]\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    **celebrities:**\n",
    "    {{CELEBRITIES}}\n",
    "    \n",
    "   Context:\n",
    "    - Each image contains a sequence of consecutive video frames, read from left to right and top to bottom.\n",
    "    - Your goal is to generate metadata that makes the video content easily discoverable through various search queries.\n",
    "    - ALL identified celebrities MUST be integrated into descriptions.\n",
    "\n",
    "    STRICT VALIDATION REQUIREMENTS:\n",
    "    1. STOP AND CHECK BEFORE OUTPUTTING:\n",
    "       - Are there any names in the \"celebrities\" field? \n",
    "       - If YES, verify these names appear in the description text\n",
    "       - If NO match found, rewrite description to include celebrity names\n",
    "       \n",
    "    2. REQUIRED FORMAT FOR DESCRIPTIONS WITH CELEBRITIES:\n",
    "       - MUST start with celebrity names and their actions\n",
    "       - Example format: \"[Celebrity Name] appears/is shown/portrays...\"\n",
    "       - NEVER output generic terms (\"a man\", \"someone\") when celebrity identity is known\n",
    "    \n",
    "    3. AUTOMATIC ERROR CHECKING:\n",
    "       If (celebrities.length > 0):\n",
    "          If (description does not contain ALL celebrity names):\n",
    "             MUST rewrite description\n",
    "    \n",
    "    Description Template When Celebrities Present:\n",
    "    \"[Celebrity Name 1] [action/appearance], [clothing/setting details]. [Additional context]. [Celebrity Name 2 if present] [their action/appearance]...\"\n",
    "\n",
    "    REQUIRED PRE-SUBMISSION CHECKS:\n",
    "    □ Are all celebrity names from <celebrities> present in description?\n",
    "    □ Does description start with a celebrity name (not generic terms)?\n",
    "    □ Are all celebrities actively described (not passively mentioned)?\n",
    "    □ Have you avoided generic terms like \"a man\" or \"someone\"?\n",
    "    \n",
    "    INCORRECT (Reject):\n",
    "    \"A man in a white shirt and tie is shown...\"\n",
    "    (When celebrities field contains \"Kevin Kilner\")\n",
    "    \n",
    "    CORRECT (Accept):\n",
    "    \"Kevin Kilner appears in a white shirt and tie...\"\n",
    "    \n",
    "    Input Description:\n",
    "    - Sequence of images representing video frames\n",
    "    - List of known celebrities (if applicable)\n",
    "\n",
    "    Step-by-step Instructions:\n",
    "    1. Analyze the visual content:\n",
    "       a. First priority: Identify any celebrities or notable individuals\n",
    "       b. Check for dark/empty frames:\n",
    "          - If frames are black or empty, use specialized template\n",
    "          - Set appropriate technical descriptors\n",
    "          - Mark confidence scores as 100 for verified empty content\n",
    "          - Use \"None\" or \"Undefined\" for inapplicable categories\n",
    "       c. If celebrities identified, prepare description using required template\n",
    "       d. Identify key objects, actions, and settings in the scene\n",
    "       e. Detect any text or graphics visible in the frames\n",
    "       f. Recognize brands, logos, or products\n",
    "    \n",
    "    2. Determine temporal aspects:\n",
    "       a. Identify any scene transitions or significant changes in the sequence\n",
    "       b. Note any recurring elements across multiple frames\n",
    "    \n",
    "    3. Synthesize a detailed description:\n",
    "       a. REQUIRED: If celebrities present, use template format\n",
    "       b. MUST start with celebrity identification and actions\n",
    "       c. Integrate setting, atmosphere, and context\n",
    "       d. Include all identified celebrities in natural narrative flow\n",
    "       e. Run pre-submission checks before finalizing\n",
    "    \n",
    "    4. Final Validation:\n",
    "       a. Run through pre-submission checklist\n",
    "       b. Verify celebrity integration in description (if applicable)\n",
    "       c. Confirm no generic terms used for identified people\n",
    "       d. For dark frames, verify all technical descriptors are accurate\n",
    "    \n",
    "    5. Special Cases Handling:\n",
    "        a. For dark/empty frames:\n",
    "           - Use technical description template\n",
    "           - Set appropriate null values\n",
    "           - Mark relevant technical indicators\n",
    "           - Note possible transition purpose\n",
    "        b. For partially visible content:\n",
    "           - Note visibility issues\n",
    "           - Describe what can be confidently identified\n",
    "           - Adjust confidence scores accordingly\n",
    "    \n",
    "    6. Final Output Preparation:\n",
    "        a. Skip the preamble; go straight into the description\n",
    "        b. Check for proper formatting and syntax\n",
    "    \"\"\".replace(\"{{CELEBRITIES}}\", celebrities)\n",
    "\n",
    "    \n",
    "    message = {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":[]\n",
    "    }\n",
    "                         \n",
    "    for composite in composite_images:\n",
    "\n",
    "        with open(composite['file'], \"rb\") as image_file:\n",
    "            image_string = image_file.read()\n",
    "\n",
    "        message[\"content\"].append({\n",
    "            \"image\":{\n",
    "                \"format\": \"jpeg\",\n",
    "                \"source\":{\n",
    "                    \"bytes\": image_string\n",
    "                }\n",
    "            }\n",
    "                \n",
    "        })\n",
    "\n",
    "    message[\"content\"].append({\n",
    "        \"text\": prompt\n",
    "    })\n",
    "\n",
    "    inference_config = {\"maxTokens\": 4096, \"topP\": 0.7, \"temperature\": 0.1}\n",
    "\n",
    "    additional_model_fields = {\n",
    "        \"inferenceConfig\": {\n",
    "             \"topK\": 20\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[message],\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "    output_message = response['output']['message']\n",
    "    \n",
    "    return output_message[\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca072a9b-c287-4fb2-8a42-be897a4a977a",
   "metadata": {},
   "source": [
    "Execute the code to generate the shot descriptions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5d741-dd4a-4be8-a7e0-691c4a312b27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08d87b-2533-4823-a3b8-b4a2edd229ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_id = \"arn:aws:bedrock:us-west-2:376678947624:inference-profile/us.amazon.nova-pro-v1:0\"\n",
    "\n",
    "for shot in sampled_shots:\n",
    "    description = get_shot_description(\n",
    "        model_id = model_id, \n",
    "        composite_images = shot['composite_images'], \n",
    "        celebrities = shot['public_figure']\n",
    "    )\n",
    "    shot['shot_description'] = description\n",
    "    time.sleep(30)\n",
    "\n",
    "print(colored(\"===== [Example caption] ======\\n\", \"green\"))\n",
    "\n",
    "example = random.choice(sampled_shots)\n",
    "example['shot_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d186e9-89d1-48b4-9b6b-91dc468a4b92",
   "metadata": {},
   "source": [
    "## Generate Embeddings for Shots\n",
    "\n",
    "This function will generate text or multi-modal embeddings base on provided model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177f3fe-2ae5-4bf7-8b28-17014b6e52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model_id, input_data):\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "\n",
    "    if 'text' in model_id:\n",
    "        body = json.dumps({\n",
    "            \"inputText\": input_data,\n",
    "            \"dimensions\": 1024,\n",
    "            \"normalize\": True\n",
    "        })\n",
    "    elif 'image' in model_id:\n",
    "        # Read image from file and encode it as base64 string.\n",
    "        with open(input_data, \"rb\") as image_file:\n",
    "            input_image = base64.b64encode(image_file.read()).decode('utf8')\n",
    "        \n",
    "        body = json.dumps({\n",
    "            \"inputImage\": input_image,\n",
    "            \"embeddingConfig\": {\n",
    "                \"outputEmbeddingLength\": 1024\n",
    "            }\n",
    "        })\n",
    "    else:\n",
    "        raise ValueError(\"Invalid embedding_type. Choose 'text' or 'image'.\")\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type,\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53d953-ea15-4d0f-9636-2e222e6303dd",
   "metadata": {},
   "source": [
    "## Building the OpenSearch Serverless Vector Index\n",
    "\n",
    "OpenSearch Serverless (OSS) is a fully managed, on-demand search and analytics service provided by Amazon Web Services (AWS). It allows users to deploy, operate, and scale OpenSearch clusters without the need for infrastructure management.\n",
    "\n",
    "An index in OpenSearch is a collection of documents that share similar characteristics. In this case, we're focusing on a vector index, which is designed to store and search vector embeddings efficiently.\n",
    "\n",
    "### Here is the Index Configuration\n",
    "\n",
    "The index contain following attributes:\n",
    "- `video_path`: Path to the video file (text field)\n",
    "- `shot_id`: Unique identifier for each shot (text field)\n",
    "- `shot_startTime`: Start time of the shot (text field)\n",
    "- `shot_endTime`: End time of the shot (text field)\n",
    "- `shot_description`: Description of the shot (text field)\n",
    "- `shot_celebrities`: Celebrities identified in the shot (text field)\n",
    "- `shot_transcript`: Audio Transcript of the shot (text field)\n",
    "\n",
    "These are metadata fields we can use to retrieve shots for each search query, as well as use them to filter results. \n",
    "\n",
    "- `shot_image_vector`: Vector representation of the shot image\n",
    "- `shot_desc_vector`: Vector representation of the shot description\n",
    "- `transcript_vector`: Vector representation of the transcript\n",
    "\n",
    "`shot_image_vector`, `transcript_vector` and `shot_desc_vector` are configured as `knn_vector` fields. You will use these two field to conduct a vector similarity search to find the closest matching camera shot corresponding to your text query or input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d4371-bd0f-4cf3-bb8c-fc8009ef4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish client connection OSS\n",
    "def get_opensearch_client(host, region):\n",
    "    host = host.split(\"://\")[1] if \"://\" in host else host\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    auth = AWSV4SignerAuth(credentials, region, \"aoss\")\n",
    "\n",
    "    oss_client = OpenSearch(\n",
    "        hosts=[{\"host\": host, \"port\": 443}],\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        pool_maxsize=20,\n",
    "    )\n",
    "\n",
    "    return oss_client\n",
    "\n",
    "\n",
    "# Create OpenSearch Severless Index\n",
    "def create_opensearch_index(oss_client, index_name, len_embedding=1024):\n",
    "\n",
    "    exist = oss_client.indices.exists(index_name)\n",
    "    if not exist:\n",
    "        print(\"Creating index\")\n",
    "        index_body = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"video_path\": {\"type\": \"text\"},\n",
    "                    \"shot_id\": {\"type\": \"text\"},\n",
    "                    \"shot_startTime\": {\"type\": \"text\"},\n",
    "                    \"shot_endTime\": {\"type\": \"text\"},\n",
    "                    \"shot_description\": {\"type\": \"text\"},\n",
    "                    \"shot_celebrities\": {\"type\": \"text\"},\n",
    "                    \"shot_transcript\": {\"type\": \"text\"},\n",
    "                    \"shot_image_vector\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": len_embedding,\n",
    "                        \"method\": {\n",
    "                            \"engine\": \"nmslib\",\n",
    "                            \"space_type\": \"cosinesimil\",\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                        },\n",
    "                    },\n",
    "                    \"shot_desc_vector\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": len_embedding,\n",
    "                        \"method\": {\n",
    "                            \"engine\": \"nmslib\",\n",
    "                            \"space_type\": \"cosinesimil\",\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                        },\n",
    "                    },\n",
    "                    \"transcript_vector\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": len_embedding,\n",
    "                        \"method\": {\n",
    "                            \"engine\": \"nmslib\",\n",
    "                            \"space_type\": \"cosinesimil\",\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            \"settings\": {\n",
    "                \"index\": {\n",
    "                    \"number_of_shards\": 2,\n",
    "                    \"knn.algo_param\": {\"ef_search\": 512},\n",
    "                    \"knn\": True,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        response = oss_client.indices.create(index_name, body=index_body)\n",
    "\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbfcdb-00f2-4d15-adcb-5f63a3d7a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"video_search_index\"\n",
    "\n",
    "oss_client = get_opensearch_client(oss_host, region)\n",
    "create_opensearch_index(oss_client, index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a687d533-7027-48c1-9999-dc82b4b3db94",
   "metadata": {},
   "source": [
    "Loop through each of the shots to create ingestion payload and ingest data to OpenSearch Serverless (OSS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119aa56-0a5f-4caa-a534-995f5fc8c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shot in sampled_shots:\n",
    "\n",
    "    # generate text embedding from description\n",
    "    shot_desc_vector = get_embedding(\n",
    "        model_id='amazon.titan-embed-text-v2:0',\n",
    "        input_data=shot['shot_description']\n",
    "    )\n",
    "\n",
    "    # generate mm embedding from composite frames\n",
    "    shot_image_vector = get_embedding(\n",
    "        model_id='amazon.titan-embed-image-v1',\n",
    "        input_data=shot['composite_images'][0]['file']\n",
    "    )\n",
    "\n",
    "    index_obj = {\n",
    "                \"video_path\": video_path,\n",
    "                \"shot_id\": shot['id'],\n",
    "                \"shot_startTime\": shot['start_ms'],\n",
    "                \"shot_endTime\": shot['end_ms'],\n",
    "                \"shot_description\": shot['shot_description'],\n",
    "                \"shot_celebrities\": shot['public_figure'],\n",
    "                \"shot_transcript\": shot['transcript'],\n",
    "                \"shot_desc_vector\": shot_desc_vector,\n",
    "                \"shot_image_vector\": shot_image_vector,\n",
    "            }\n",
    "\n",
    "    # generate text embedding from transcript\n",
    "    if shot['transcript']:\n",
    "        \n",
    "        transcript_vector = get_embedding(\n",
    "            model_id='amazon.titan-embed-text-v2:0',\n",
    "            input_data=shot['transcript']\n",
    "        )\n",
    "        \n",
    "        index_obj[\"transcript_vector\"] = transcript_vector\n",
    "        \n",
    "    #build the payload to index in OSS\n",
    "    payload = json.dumps(index_obj)\n",
    "    response = oss_client.index(\n",
    "                    index=index_name,\n",
    "                    body=payload,\n",
    "                    params={\"timeout\": 60},\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3fc17-e5c4-42e7-ba00-eff91e92ee5f",
   "metadata": {},
   "source": [
    "## Perform Video Semantic Search\n",
    "\n",
    "We will wait to make sure the inserted data in OpenSearch is ready to be searched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073b6ac-a09c-4a36-bd65-357671105ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting for the recent inserted data to be searchable in OpenSearch...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        result = oss_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "        if result['hits']['total']['value'] >= len(sampled_shots):\n",
    "            print(\"\\nData is now available for search!\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdc04d-d906-490f-ad9d-03dd559fe24b",
   "metadata": {},
   "source": [
    "Demonstrate search using natural language queries. By default, a question is randomly sampled from the Question Bank. The search then combines both visual (`shot_desc_vector`) and audio (`transcript_vector`) data from the OSS index. The process applies content weighting to prioritize visual (75%) over audio transcriptions (25%) for optimized results in these types of videos. This combined search example illustrates how these parameters can be adjusted to better match different user search intents, improving overall search relevance and user satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a8fa9-4d03-47f3-917e-9fc9a93fc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_bank = {\n",
    "    \"Netflix_Open_Content_Meridian.mp4\": [\n",
    "            \"Scott driving a car\",\n",
    "            \"Elyse staring through the rear view mirror\",\n",
    "            \"Kevin opening a car door\",\n",
    "            \"Lightning strike from the sky\"\n",
    "        ]    \n",
    "}\n",
    "# check if questions are available for the video\n",
    "assert video['path'] in question_bank, f\"****[{video['path']}]*** is not a supported video.\"\n",
    "\n",
    "# Sample a question from the question bank. You can change to use your own.\n",
    "user_query = random.choice(question_bank[video['path']])\n",
    "\n",
    "print(\"Sampled query: \", colored(user_query, \"green\"))\n",
    "\n",
    "query_embedding = get_embedding('amazon.titan-embed-text-v2:0', user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bee18-b641-4a08-a9fc-a6c731214630",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_query = {\n",
    "        \"size\": 10,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"should\": [\n",
    "                    {\n",
    "                        \"script_score\": {\n",
    "                            \"query\": {\"match_all\": {}},\n",
    "                            \"script\": {\n",
    "                                \"lang\": \"knn\",\n",
    "                                \"source\": \"knn_score\",\n",
    "                                \"params\": {\n",
    "                                    \"field\": \"shot_desc_vector\",\n",
    "                                    \"query_value\": query_embedding,\n",
    "                                    \"space_type\": \"cosinesimil\",\n",
    "                                },\n",
    "                            },\n",
    "                            \"boost\": 3.0\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"script_score\": {\n",
    "                            \"query\": {\"match_all\": {}},\n",
    "                            \"script\": {\n",
    "                                \"lang\": \"knn\",\n",
    "                                \"source\": \"knn_score\",\n",
    "                                \"params\": {\n",
    "                                    \"field\": \"transcript_vector\",\n",
    "                                    \"query_value\": query_embedding,\n",
    "                                    \"space_type\": \"cosinesimil\",\n",
    "                                },\n",
    "                            },\n",
    "                            \"boost\": 1.0\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"minimum_should_match\": 1\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\n",
    "            \"video_path\",\n",
    "            \"shot_id\",\n",
    "            \"shot_startTime\",\n",
    "            \"shot_endTime\",\n",
    "            \"shot_description\",\n",
    "            \"shot_celebrities\",\n",
    "            \"shot_transcript\",\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1323c3c-3dc0-4745-94cf-91db90ab32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = oss_client.search(body=aoss_query, index=index_name)\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "\n",
    "responses = []\n",
    "for hit in hits:\n",
    "    if hit[\"_score\"] >= 0:  # Set score threshold\n",
    "        responses.append(\n",
    "            {\n",
    "                \"video_path\": hit[\"_source\"][\"video_path\"],\n",
    "                \"shot_id\": hit[\"_source\"][\"shot_id\"],\n",
    "                \"shot_startTime\": hit[\"_source\"][\"shot_startTime\"],\n",
    "                \"shot_endTime\": hit[\"_source\"][\"shot_endTime\"],\n",
    "                \"shot_description\": hit[\"_source\"][\"shot_description\"],\n",
    "                \"shot_celebrities\": hit[\"_source\"][\"shot_celebrities\"],\n",
    "                \"shot_transcript\": hit[\"_source\"][\"shot_transcript\"],\n",
    "                \"score\": hit[\"_score\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651fbe9f-5d65-4bae-87ac-efc9bf17db36",
   "metadata": {},
   "source": [
    "Display the top `x` search results. The helper function below render the shots independently and as part of the original video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a1096-0014-42c3-887a-6f8aa5089253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_with_original_video(top_hit):\n",
    "    \n",
    "    video_path = top_hit['video_path']\n",
    "    video_start = top_hit['shot_startTime']/1000\n",
    "    \n",
    "    display(HTML(f\"\"\"\n",
    "    <video alt=\"test\" controls id=\"{top_hit['shot_id']}\" width=\"100\" >\n",
    "      <source src=\"{video_path}\">\n",
    "    </video>\n",
    "    \n",
    "    <script>\n",
    "    video = document.getElementById(\"{top_hit['shot_id']}\")\n",
    "    video.currentTime = {video_start};\n",
    "    </script>\n",
    "    \"\"\"))\n",
    "    \n",
    "    \n",
    "def display_shot_segment_results(response, top_results=2):\n",
    "\n",
    "    css_style = \"\"\"\n",
    "    <style>\n",
    "        .video-container {\n",
    "            display: flex;\n",
    "            justify-content: space-around;\n",
    "            flex-wrap: wrap;\n",
    "        }\n",
    "        .video {\n",
    "            flex: 1;\n",
    "            min-width: 200px;\n",
    "            margin: 10px;\n",
    "        }\n",
    "        video {\n",
    "            width: 100%;\n",
    "            height: auto;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    html_content = \"<div class='video-container'>\\n\"\n",
    "    \n",
    "    for idx in range(top_results):\n",
    "        # convert format of timestamps\n",
    "        video_start = responses[idx]['shot_startTime']/1000\n",
    "        video_end = responses[idx]['shot_endTime']/1000\n",
    "    \n",
    "        converted_start = str(datetime.timedelta(seconds = video_start))\n",
    "        converted_end = str(datetime.timedelta(seconds = video_end))\n",
    "        output_file = f\"shot-{responses[idx]['shot_id']}.mp4\"\n",
    "        _ = subprocess.run(\n",
    "            [\n",
    "                \"/usr/bin/ffmpeg\",\n",
    "                \"-ss\",\n",
    "                converted_start,\n",
    "                \"-to\",\n",
    "                converted_end,\n",
    "                \"-i\",\n",
    "                responses[idx]['video_path'], # path to video\n",
    "                \"-c\",\n",
    "                \"copy\",\n",
    "                output_file,\n",
    "            ],\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        html_content += f\"\"\"\n",
    "            <div class=\"video\">\n",
    "                <h5>Shot Id: {responses[idx]['shot_id']}, Time Range: {video_start} ms - {video_end} ms</p>\n",
    "                <video controls>\n",
    "                    <source src=\"{output_file}\" type=\"video/mp4\">\n",
    "                    Your browser does not support the video tag.\n",
    "                </video>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "    # render the shots\n",
    "    html_content += \"</div>\"\n",
    "    \n",
    "    display(HTML(css_style + html_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1c2e0-06ca-4ed3-83c1-9d2f1d898f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"====== [TOP results] =======\", 'green'))\n",
    "display_shot_segment_results(responses, top_results=3)\n",
    "\n",
    "print(colored(\"\\n====== [Display top hit as part Of original video] =======\\n\", 'green'))\n",
    "\n",
    "top_hit = responses[0]\n",
    "\n",
    "video_start = top_hit['shot_startTime']/1000\n",
    "video_end = top_hit['shot_endTime']/1000\n",
    "\n",
    "print(f\"Shot Id: {top_hit['shot_id']}, Time Range: {video_start} ms - {video_end} ms\")\n",
    "render_with_original_video(top_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8579f37-81b6-4b45-b30e-d6b5452a0585",
   "metadata": {},
   "source": [
    "### Multi-Modal Video Search\n",
    "\n",
    "In content analysis and video editing workflows, a video producer may come across a specific frame or image that perfectly captures a key moment, but they need to locate its position within hours of raw footage. With multi-modal video search using this Generative AI technique, the producer can input the frame or image and quickly pinpoint the precise timestamp where the frame occurs.\n",
    "\n",
    "In the next search example, you will randomly sample a frame from available shots, and then use the frame image to identify the shot in the video.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5374481-3a36-49d9-9cc5-bcd6967a73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_image(shots):\n",
    "    shot = random.choice(shots) if shots else None\n",
    "    frame_locations = shot['composite_images'][0]['layout']\n",
    "    frame_info = random.choice(frame_locations) if frame_locations else None\n",
    "    return frame_info[0]\n",
    "\n",
    "random_frame = random_sample_image(sampled_shots)\n",
    "image = Image.open(random_frame)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a33089-5044-4543-bbf3-cc98d354bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding = get_embedding('amazon.titan-embed-image-v1', random_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278349e-f96d-4a80-943b-32d8d4fe3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_query = {\n",
    "        \"size\": 10,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"bool\": {\"should\": []}},\n",
    "                \"script\": {\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"shot_image_vector\",\n",
    "                        \"query_value\": image_embedding,\n",
    "                        \"space_type\": \"cosinesimil\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\n",
    "            \"video_path\",\n",
    "            \"shot_id\",\n",
    "            \"shot_startTime\",\n",
    "            \"shot_endTime\",\n",
    "            \"shot_description\",\n",
    "            \"shot_celebrities\",\n",
    "            \"shot_transcript\",\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d0858-63a3-449b-9829-a13716487db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = oss_client.search(body=aoss_query, index=index_name)\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "\n",
    "responses = []\n",
    "for hit in hits:\n",
    "    if hit[\"_score\"] >= 0:  # Set score threshold\n",
    "        responses.append(\n",
    "            {\n",
    "                \"video_path\": hit[\"_source\"][\"video_path\"],\n",
    "                \"shot_id\": hit[\"_source\"][\"shot_id\"],\n",
    "                \"shot_startTime\": hit[\"_source\"][\"shot_startTime\"],\n",
    "                \"shot_endTime\": hit[\"_source\"][\"shot_endTime\"],\n",
    "                \"shot_description\": hit[\"_source\"][\"shot_description\"],\n",
    "                \"shot_celebrities\": hit[\"_source\"][\"shot_celebrities\"],\n",
    "                \"shot_transcript\": hit[\"_source\"][\"shot_transcript\"],\n",
    "                \"score\": hit[\"_score\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc4dc6-c525-4d50-81b1-48b18a30b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"====== [TOP results] =======\", 'green'))\n",
    "display_shot_segment_results(response, top_results=3)\n",
    "\n",
    "print(colored(\"\\n====== [Display top hit as part Of original video] =======\\n\", 'green'))\n",
    "\n",
    "top_hit = responses[0]\n",
    "\n",
    "video_start = top_hit['shot_startTime']/1000\n",
    "video_end = top_hit['shot_endTime']/1000\n",
    "\n",
    "print(f\"Shot Id: {top_hit['shot_id']}, Time Range: {video_start} ms - {video_end} ms\")\n",
    "render_with_original_video(top_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289023ae-6fe5-45c9-8420-d161dd8bd1a3",
   "metadata": {},
   "source": [
    "## Clean Up\n",
    "Uncomment the code below to remove the vector index we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bd7ce-044e-4a54-a7c3-4b3bedbb9789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     response = oss_client.indices.delete(index=index_name)\n",
    "#     print(f\"Index '{index_name}' deleted successfully\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error deleting index '{index_name}': {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3595a36-7271-4076-8131-da13bb99dfb5",
   "metadata": {},
   "source": [
    "# What's Next?\n",
    "\n",
    "You can try another use case or, if you are done, continue to the [Additional Resources](09-resources.ipynb) lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85544b34-2954-449b-9eb0-3faff010d39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
