{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c84a641-be01-4cd5-8b24-f175f16121c4",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ab7a0-5fd1-4418-a0c2-4e401ad47eef",
   "metadata": {},
   "source": [
    "## Workshop overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314b5cb-335e-48d2-b26a-a1ab92291f36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Welcome to the Video Understanding on AWS workshop!\n",
    "\n",
    "The workshop is organized into two main parts: 1. Media Analysis using Bedrock Data Automation and 2. Media Analysis using Amazon Nova. Read on to get an overview of the different sections.  Part 1 and Part 2 can be run independently after running this notebook.\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "Before running the main workshop, you'll set up the notebook environment using this notebook.\n",
    "\n",
    "**Part 1: Media Analysis using Bedrock Data Automation (BDA):**\n",
    "\n",
    "The notebooks in this section give an overview of BDA APIs and use cases.  They can be run in any order.  \n",
    "\n",
    "1. [Extract and analyze a movie with BDA](1-media-analysis-using-bda/01-extract-analyze-a-movie.ipynb)\n",
    "2. [Contextual Ad overlay](1-media-analysis-using-bda/02-contextual-ad-overlay.ipynb)\n",
    "\n",
    "**Part 2: Media Analysis using Amazon Nova:**\n",
    "\n",
    "In the foundation notebooks, you'll set up the notebook environment, prepare the sample video by breaking it down into clips, and you will experiment with using Foundation models to generate insights about video clips.  In the second part of the workshop, you will use the foundations to solve different video understanding use cases.  The use cases are independent and can be run in any order.\n",
    "\n",
    "**Foundation (required before running use cases)**\n",
    "1. [Visual video segments: frames, shots and scenes](2-media-analysis-using-amazon-nova/01A-visual-segments-frames-shots-scenes.ipynb) (20 minutes)\n",
    "2. [Audio segments](2-media-analysis-using-amazon-nova/01B-audio-segments.ipynb) (10 minutes)\n",
    "\n",
    "**Use cases (optional, run in any order):**\n",
    "\n",
    "After running the Foundations notebooks, you can choose any use case.  If you are running at an AWS Workshop event, you will be able to complete foundations plus one use case in a 2 hour session:\n",
    "\n",
    "* [Ad break detection and contextual Ad tartgeting](2-media-analysis-using-amazon-nova/02-ad-breaks-and-contextual-ad-targeting.ipynb) (20 minutes) - identify opportunities for ad insertion.  Use a standard taxonomy to match video content to ad content.\n",
    "* [Video summarization](2-media-analysis-using-amazon-nova/03-video-summarization.ipynb) (20 minutes) - generate short form videos from a longer video\n",
    "* [Semantic video search](2-media-analysis-using-amazon-nova/04-semantic-video-search.ipynb) (20 minutes) - search video using images and natural language to find relevant clips\n",
    "\n",
    "**Resources**\n",
    "\n",
    "The activities in this workshop are based on AWS Solution Guidance.  The [Additional Resources](./09-resources.ipynb) lab contains links to relevant reference architectures, code samples and blog posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce212e-9c89-4b32-8327-52fae47b2ff5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Install ffmpeg and python packages\n",
    "\n",
    "- ffmpeg for video and image processing\n",
    "- faiss for vector store\n",
    "- webvtt-py for parsing subtitle file\n",
    "- termcolor for formatting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab951b69-de93-47d8-9d1a-1ee1cbe334dc",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## install ffmpeg\n",
    "!sudo apt update -y && sudo apt-get -y install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9bef1-db74-4935-b033-a25e4738e7c6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Make sure ffmpeg is installed\n",
    "!which ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62880031-88de-4ba3-8267-aa3837d3df95",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626bb1bf-f941-4b26-be79-3ca659c87639",
   "metadata": {},
   "source": [
    "## Get SageMaker default resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b8b333-7644-4665-95b5-85ce17eae42b",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_resources = {}\n",
    "#sagemaker_resources[\"session\"] = sagemaker.Session()\n",
    "sagemaker_resources[\"role\"] = sagemaker.get_execution_role()\n",
    "sagemaker_resources[\"region\"] = sagemaker.Session()._region_name\n",
    "\n",
    "print(sagemaker_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb318ff-0b5c-4095-bddf-4eb947baf170",
   "metadata": {},
   "source": [
    "# Setup session AWS resources\n",
    "\n",
    "If you are running this from an AWS hosted event, the AWS resources have been pre-created for you and you will load them from the CloudFormation outputs using the cell below.  If you are running in your own account, you will need to set the following variables from the resources you created manually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a018cb6-c58c-41b5-bcdf-263ff932bba2",
   "metadata": {},
   "source": [
    "## Get CloudFormation stack outputs (AWS hosted event only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73ce3b-4c34-4fff-8cff-642dcd0c3352",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from IPython.display import JSON\n",
    "cf = boto3.client(service_name=\"cloudformation\")\n",
    "stack = response = cf.describe_stacks(StackName='workshop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f450a-d480-45d9-a518-d90cb14027ee",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6771eae-5fc9-408e-8464-1ed9726b9955",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = {}\n",
    "session['bucket'] = next(item[\"OutputValue\"] for item in stack['Stacks'][0]['Outputs'] if item[\"OutputKey\"] == \"S3BucketName\")\n",
    "session['MediaConvertRole'] = next(item[\"OutputValue\"] for item in stack['Stacks'][0]['Outputs'] if item[\"OutputKey\"] == \"MediaConvertRole\")\n",
    "session[\"AOSSCollectionEndpoint\"] = next(item[\"OutputValue\"] for item in stack['Stacks'][0]['Outputs'] if item[\"OutputKey\"] == \"AOSSCollectionEndpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacbfe19-95f2-4e6b-8a57-320582d44bff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Find Amazon Q Developer\n",
    "\n",
    "Jupyter notebooks in SageMaker Studio have Amazon Q Developer enabled.  \n",
    "\n",
    "1. To use Q Developer click on the Q Developer chat icon in the left sidebar menu. The active side panel should now be Amazon Q Developer.\n",
    "<br></br>\n",
    "<img src=\"static/images/00-qdev-sidebar1.png\" alt=\"Q Developer Sidebar\" style=\"width: 600px;\"/>\n",
    "<br></br>\n",
    "5. Try it out by asking a question.  For example, you could ask: `What kinds of questions can Q developer answer? Be brief.` You should get a response like this:\n",
    "<br></br>\n",
    "<img src=\"static/images/00-qdev-skills1.png\" alt=\"Q Developer Skills\" style=\"width: 600px;\"/>\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb045c80-93ee-45dd-9c2f-d510e1cc29bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Throughout this workshop, you can use Q when you encounter errors or have questions about the code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4e668-e7bc-4594-b9e8-d53c9c0834c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Save variables we will use in other notebooks\n",
    "\n",
    "We will use this data in the next labs. In order to use this data we will store these variables so subsequent notebooks can use this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be070591-e890-4147-97ed-c3f77af4eb35",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store sagemaker_resources\n",
    "%store session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970ade49-c037-46d9-b918-cc99abbcac55",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Continue to the next section of the workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2104fa1b-4880-44c7-b715-a3c1ee44d4a1",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "From here, you can choose to go to either Part1 to work with Bedrock Data Automation or Part 2 to work with Amazon Nova workflows.  \n",
    "\n",
    "1. [Start of Part1: Extract and analyze a movie with BDA](1-media-analysis-using-bda/01-extract-analyze-a-movie.ipynb)\n",
    "2. [Start of Part 2: Visual video segments: frames, shots and scenes](2-media-analysis-using-amazon-nova/01A-visual-segments-frames-shots-scenes.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9694f-5b19-4b41-a0a7-ab7a888a5260",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
