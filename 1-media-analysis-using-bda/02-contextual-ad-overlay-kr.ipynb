{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "259074a6-a277-4247-a8d1-faa44d0020bf",
   "metadata": {},
   "source": [
    "# Contextual ad overlays\n",
    "\n",
    "\n",
    "Contextual ad targeting is an advertising strategy that focuses on displaying ads based on the content and context where the ad will appear, rather than relying on user personal data or behavior.  This approach has gained renewed importance in the post-GDPR era as the digital advertising industry moves away from third-party cookies and personal data collection, making it an effective alternative to behavioral targeting while maintaining ad relevance and effectiveness.\n",
    "\n",
    "Amazon Bedrock Data Automation (BDA) provides features to help automate the process of contextual ad targeting for video content. With BDA, you can analyze the video's content to identify different scenes and generate contextual metadata for each scene. This metadata includes:\n",
    "\n",
    "* Classification of each scene using the IAB Content Taxonomy - The IAB Content Taxonomy is a standard used by advertisers to categorize content for automated ad placement through Ad Decision Servers.\n",
    "\n",
    "By leveraging the IAB Content Taxonomy, you can match advertisements to the contextual information of each video scene. This allows you to create more effective and relevant advertising experiences that are tailored to the video's content.  The image below is a screenshot of a sample video with ad overlays.\n",
    "\n",
    "![Example of an ad overlay](static/images/ad-overlay-example.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3fd90-483f-4db7-9a09-744d299ef88e",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "This hands-on workflow uses AWS services from SageMaker.  You will use BDA to analyze a sample video to identify scenes in the video where ads can be placed that adhere to brand safety guidelines.  Then you will select the best ad for each opportunity by matching the IAB category for an ad to the IAB category for the scene.\n",
    "\n",
    "![Workflow diagram](static/images/ad-overlays.drawio.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87659b5-90c4-40a4-9c03-931d70ab8955",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d70a327-cd57-4290-96f5-eb6b7ccc9b9d",
   "metadata": {},
   "source": [
    "### Retrieve saved values from previous notebooks\n",
    "\n",
    "To run this notebook, you need to have run the previous notebook: [00_prerequisites.ipynb](./00-prequisites.ipynb), where you installed package dependencies and gathered some information from the SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01472477-b71c-44bc-85f5-0e85acdadfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf31c6e-182c-4998-83ce-f60ed4f53b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install itables==2.2.4 PyPDF2==3.0.1 --upgrade -qq\n",
    "%pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc3186-7df3-4e5f-ae7d-2522ce275209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_bucket = session[\"bucket\"]\n",
    "region = sagemaker_resources[\"region\"]\n",
    "data_prefix = \"bda/video\"\n",
    "output_prefix = \"bda/video/ouput\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94080f3-7a23-4b04-898d-57be31c638a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "import utils\n",
    "from IPython.display import Video, Image, display\n",
    "\n",
    "bda_client = boto3.client('bedrock-data-automation')\n",
    "bda_runtime_client = boto3.client('bedrock-data-automation-runtime')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "#access account id\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "default_profile_arn = f\"arn:aws:bedrock:{region}:{account_id}:data-automation-profile/us.data-automation-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa554a34-ae81-4802-9ca5-b4a40e759f7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T00:31:53.506949Z",
     "iopub.status.busy": "2025-01-21T00:31:53.506589Z",
     "iopub.status.idle": "2025-01-21T00:31:53.513035Z",
     "shell.execute_reply": "2025-01-21T00:31:53.511550Z",
     "shell.execute_reply.started": "2025-01-21T00:31:53.506925Z"
    }
   },
   "source": [
    "## Create a BDA project\n",
    "To start a BDA job, you need a BDA project, which organizes both standard and custom output configurations. This project is reusable, allowing you to apply the same configuration to process multiple videos that share the same settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431eeea0-12c1-4cd7-8e40-f8c3d8b3f8be",
   "metadata": {},
   "source": [
    "In the code snippet below, we create a BDA project with standard output configurations for video modality. These configurations can be tailored to extract only the specific information you need. In this lab, we will enable the below video outputs:\n",
    "- Scene summary\n",
    "- Content moderation (visual and audio)\n",
    "- IAB taxonomy classification of scenes\n",
    "\n",
    "For a complete API reference for creating a BDA project, refer to this [document](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-data-automation/client/create_data_automation_project.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2510f5-f4f2-4fdd-ac96-4d9edbeeba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bda_client.create_data_automation_project(\n",
    "    projectName=f'bda-workshop-video-project-moderation-{str(uuid.uuid4())[0:4]}',\n",
    "    projectDescription='BDA workshop video sample project - content moderation',\n",
    "    projectStage='DEVELOPMENT',\n",
    "    standardOutputConfiguration={\n",
    "        'video': {\n",
    "            'extraction': {\n",
    "                'category': {\n",
    "                    'state': 'ENABLED',\n",
    "                    'types': ['CONTENT_MODERATION', 'TRANSCRIPT']\n",
    "                },\n",
    "                'boundingBox': {\n",
    "                    'state': 'DISABLED'\n",
    "                }\n",
    "            },\n",
    "            'generativeField': {\n",
    "                'state': 'ENABLED',\n",
    "                'types': [\n",
    "                    'CHAPTER_SUMMARY', 'IAB'\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1dfd78-a072-4375-b970-d2650909f0de",
   "metadata": {},
   "source": [
    "The `create_data_automation_project` API will return the project ARN, which we will use it to invoke the video analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824ab6c-5249-4364-927c-c5def65a32a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_project_arn = response.get(\"projectArn\")\n",
    "print(\"BDA video project ARN:\", video_project_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b81791-e597-457f-a375-265bc0711525",
   "metadata": {},
   "source": [
    "## Start an asynchronous BDA task to extract and analyze a video\n",
    "In this section, we will use a sample video contains unsafe content, and extract and analyze it using BDA, applying the configuration defined in the BDA project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd5d08-0be9-495e-bdc3-27e2b68c0e55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-21T01:16:19.221297Z",
     "iopub.status.busy": "2025-01-21T01:16:19.220673Z",
     "iopub.status.idle": "2025-01-21T01:16:19.225680Z",
     "shell.execute_reply": "2025-01-21T01:16:19.224891Z",
     "shell.execute_reply.started": "2025-01-21T01:16:19.221270Z"
    }
   },
   "source": [
    "### Prepare the sample video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bad7889-a3a7-4c82-9766-eb5ecb94acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_video_path = './NetflixMeridian.mp4'\n",
    "url = \"https://ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0.s3.us-west-2.amazonaws.com/7db2455e-0fa6-4f6d-9973-84daccd6421f/Netflix_Open_Content_Meridian.mp4\"\n",
    "!curl \"https://ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0.s3.us-west-2.amazonaws.com/7db2455e-0fa6-4f6d-9973-84daccd6421f/Netflix_Open_Content_Meridian.mp4\" --output NetflixMeridian.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b8628-c692-4385-9725-48c4ea3b70cc",
   "metadata": {},
   "source": [
    "Let's display the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd61670-4a65-471a-a75b-14525d923af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "Video(sample_video_path, width=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591cf849-11ec-4c37-a3dc-e1bb5c51c92e",
   "metadata": {},
   "source": [
    "To analyze the video using BDA, we need to upload it to an S3 bucket that BDA can access. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717c142-df4d-4d34-8a98-8f36a5671fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_key = f'{data_prefix}/{sample_video_path.split(\"/\")[-1]}'\n",
    "s3_client.upload_file(sample_video_path, data_bucket, s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b33c9-a23c-4dd3-8f66-22d169e598b7",
   "metadata": {},
   "source": [
    "### Start BDA task\n",
    "We will now invoke the BDA API to process the uploaded video. You need to provide the BDA project ARN that we created at the beginning of the lab and specify an S3 location where BDA will store the output results.\n",
    "\n",
    "For a complete API reference for invoke a BDA async task, refer to this [document](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-data-automation-runtime/client/invoke_data_automation_async.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681293ae-0023-4eac-9970-af61ec934e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bda_runtime_client.invoke_data_automation_async(\n",
    "    inputConfiguration={\n",
    "        's3Uri': f's3://{data_bucket}/{s3_key}'\n",
    "    },\n",
    "    outputConfiguration={\n",
    "        's3Uri': f's3://{data_bucket}/{output_prefix}'\n",
    "    },\n",
    "    dataAutomationConfiguration={\n",
    "        'dataAutomationProjectArn': video_project_arn,\n",
    "        'stage': 'DEVELOPMENT'\n",
    "    },\n",
    "    notificationConfiguration={\n",
    "        'eventBridgeConfiguration': {\n",
    "            'eventBridgeEnabled': False\n",
    "        }\n",
    "    },\n",
    "    dataAutomationProfileArn=default_profile_arn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c8b65a-1e4e-4dc1-b011-254e7d1cc941",
   "metadata": {},
   "source": [
    "The `invoke_data_automation_async` API is asynchronous. It returns an invocation task identifier, `invocationArn`. We can then use another API `get_data_automation_status` to monitor the task's status until it completes.\n",
    "\n",
    "> In production workloads, an event-driven pattern is recommended. Allow BDA to trigger the next step once the task is complete. This can be achieved by configuring the notificationConfiguration in the invoke task, which will send a notification to a subscribed AWS service, such as a Lambda function. Alternatively, you can set up an S3 trigger on the bucket where BDA will drop the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61061730-ded4-4852-bbf4-d18ab1168c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "invocation_arn = response.get(\"invocationArn\")\n",
    "print(\"BDA task started:\", invocation_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13518bf-15eb-4e83-bc4b-955b66974457",
   "metadata": {},
   "source": [
    "In this lab, we will use the loop below to monitor the task by calling the `get_data_automation_status` API every 5 seconds until the task is complete.\n",
    "\n",
    "This video will take less than 5 minutes to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb9c48c-9347-43ba-8148-1ea9c29479fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "status, status_response = None, None\n",
    "while status not in [\"Success\",\"ServiceError\",\"ClientError\"]:\n",
    "    status_response = bda_runtime_client.get_data_automation_status(\n",
    "        invocationArn=invocation_arn\n",
    "    )\n",
    "    status = status_response.get(\"status\")\n",
    "    clear_output(wait=True)\n",
    "    print(f\"{datetime.now().strftime('%H:%M:%S')} : BDA video task: {status}\")\n",
    "    time.sleep(5)\n",
    "\n",
    "output_config = status_response.get(\"outputConfiguration\",{}).get(\"s3Uri\")\n",
    "print(\"Ouput configuration file:\", output_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c617c2-3a71-43dd-b996-d7b9a4d86f08",
   "metadata": {},
   "source": [
    "## Access the BDA analysis result\n",
    "The `get_data_automation_status` API returns an S3 URI containing the result configuration, which provides the S3 location where BDA outputs the extraction results. We will then parse this file to retrieve the result path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e1adb7-1620-491f-adb1-b616451d62be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config_data = utils.read_json_on_s3(output_config,s3_client)\n",
    "print(json.dumps(config_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc0557-b53d-4dc4-ae1c-05de559093e7",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "As shown above, the BDA output configuration file contains metadata about the BDA result, including the job ID, status, modality, and the S3 location of the actual result JSON. We will now download this result file to verify the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc93edb4-e459-4c00-aeb6-31577240aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "\n",
    "result_uri = config_data[\"output_metadata\"][0][\"segment_metadata\"][0][\"standard_output_path\"]\n",
    "result_data = utils.read_json_on_s3(result_uri,s3_client)\n",
    "\n",
    "JSON(result_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2b413-3f1c-4225-8f60-01c00a256110",
   "metadata": {},
   "source": [
    "## Review the result\n",
    "The BDA video analysis results provide a detailed breakdown of information, organized by video and scene levels. \n",
    "> A video scene is a sequence of shots that form a coherent unit of action or narrative within the video.\n",
    "\n",
    "Take a moment to view the details of the metadata for one of the scenes.  Scenes are video segments, so they have a start time, an end time and a duration.  These segment timestamps can be used to determine when to display an ad to coincide with a scene.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed25264e-d3f8-4074-8c58-d614fbaaa73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(result_data['chapters'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c503928-cb06-4acb-bc1d-48ea3979fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = result_data['chapters'][9]['start_timestamp_millis']/1000\n",
    "end = result_data['chapters'][9]['end_timestamp_millis']/1000\n",
    "shot_url = f'{url}#t={start},{end}'\n",
    "Video(url=shot_url, width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba31f0-321e-476f-a679-2ecec7b4a2c5",
   "metadata": {},
   "source": [
    "### IAB Categories\n",
    "\n",
    "The IAB categories, `iab_categories`, have a label and a level, so that items in the same label hierarchy can be matched at different levels of detail.  For example, here are the first two levels (aka Tiers) of the IAB taxonomy for Attractions:\n",
    "\n",
    "![IAB Attractions](static/images/IAB-Attractions-1and2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633dac97-0a4f-44f5-b5b7-be5ccf139442",
   "metadata": {},
   "source": [
    "The BDA standard output for scenes contains the IAB category classification of each scene where a valid categorizations could be found.  If no suitable category is found, BDA will leave the content of the iab_categories empty.  The IAB categories and levels BDA generated for the scenes in the sample video are displayed below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00019ab8-b5a1-45f3-8844-dbd8f8d0e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IAB Categories\")\n",
    "for chapter in result_data['chapters']:\n",
    "    chapter_str = f'==Scene {chapter[\"chapter_index\"]}: '\n",
    "    for iab_cat in chapter['iab_categories']:\n",
    "        chapter_str = (f'{chapter_str} ({ iab_cat[\"category\"] }, { iab_cat[\"taxonomy_level\"] })')\n",
    "    print(chapter_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6335338a-6350-4ee2-af24-d3f0cd996651",
   "metadata": {},
   "source": [
    "## Use IAB Categories to match scenes to ads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb08683-a545-4d87-b0f1-b5e42f9d0b01",
   "metadata": {},
   "source": [
    "It is a straight forward process to match ads to scenes if they share the same content taxonomy.  Imagine we have the following list of ad images we can use for overlays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f79c697-2152-409e-b1e3-528381360ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ads = {\n",
    "    \"Automotive\": {\n",
    "        \"overlay_image\": \"static/ads/CarAd.png\",\n",
    "        \"level\": 1\n",
    "    },\n",
    "    \"Business and Finance\": {\n",
    "        \"overlay_image\": \"static/ads/BankAd.png\",\n",
    "        \"level\": 1},\n",
    "    \"Travel\": {\n",
    "        \"overlay_image\": \"static/ads/CruiseAd.png\",\n",
    "        \"level\": 1\n",
    "    },\n",
    "    \"Sports\": {\n",
    "        \"overlay_image\": \"static/ads/SportsAd.png\", \n",
    "        \"level\": 1\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06439ea2-1855-4224-8db9-1b4cbd134244",
   "metadata": {},
   "source": [
    "### Match ads to scenes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92662d44-ef14-4d85-835f-c75e05981031",
   "metadata": {},
   "source": [
    "We'll loop over the scenes, looking for scenes that have categories from the level 1 IAB taxonomy.  If you have a lot of potentials ads, you could choose to use a lower level of the taxonomy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff781da-9e1e-41f7-9a6a-301a10fa64ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "def create_video_with_overlay(url, overlay_image):\n",
    "    return HTML(f'''\n",
    "        <div class=\"video-container\" style=\"position: relative; width: 640px; height: 360px;\">\n",
    "            <video width=\"640\" height=\"360\" controls>\n",
    "                <source src=\"{url}\" type=\"video/mp4\">\n",
    "                Your browser does not support the video tag.\n",
    "            </video>\n",
    "            <img class=\"overlay\" src=\"{overlay_image}\" style=\"\n",
    "                position: absolute;\n",
    "                bottom: 0;\n",
    "                left: 0;\n",
    "                width: 100%;\n",
    "                height: 25%;  /* Takes up bottom 25% of video height */\n",
    "                opacity: 1;\n",
    "                pointer-events: none;\n",
    "                object-fit: cover;  /* Ensures image covers the area properly */\n",
    "            \">\n",
    "        </div>\n",
    "    ''')\n",
    "\n",
    "LEVEL = 1\n",
    "for chapter in result_data['chapters']:\n",
    "    if len(chapter['iab_categories']) > 0:\n",
    "        print(f\"\\n===== SCENE: {chapter['chapter_index']}\\n\")\n",
    "        start = result_data['chapters'][chapter['chapter_index']]['start_timestamp_millis']/1000\n",
    "        end = result_data['chapters'][chapter['chapter_index']]['end_timestamp_millis']/1000\n",
    "        shot_url = f'{url}#t={start},{end}'\n",
    "        \n",
    "        for iab_cat in chapter['iab_categories']:\n",
    "            if iab_cat['taxonomy_level'] == LEVEL:\n",
    "                print(f\"== Matching ad found for category: {iab_cat['category']}\\n\")\n",
    "                try:\n",
    "                    overlay_image = ads[iab_cat['category']]['overlay_image']\n",
    "                    display(create_video_with_overlay(shot_url, overlay_image))\n",
    "                except KeyError as e:\n",
    "                    print(f\"Error: Could not find key in dictionary: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ebc676-e867-4155-b136-556943b7a4b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Summary\n",
    "In this lab, we use BDA to extract and analyze a sample video to detect scenes in the video and then determine the IAB content taxonomy for the scene content so we could match ads to the content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5e1336-3ae1-49b5-8982-811637e22fe7",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "Delete the BDA project, blueprint, image, and result from S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d245e595-6a2f-41b2-9d7c-4961a18ba121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete BDA project\n",
    "response = bda_client.delete_data_automation_project(\n",
    "    projectArn=video_project_arn\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f0b6ab-cc34-40ed-9191-ae9a0e395262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete uploaded image from S3\n",
    "s3_client.delete_object(Bucket=data_bucket, Key=s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a5fc1",
   "metadata": {},
   "source": [
    "## Continue to the next section of the workshop\n",
    "\n",
    "You can either go on to Part 2 to work with Amazon Nova workflows or conclude the workshop by moving to the Additional Resources and Cleanup sections.\n",
    "\n",
    "1. Continue to [Start of Part 2: Visual video segments: frames, shots and scenes](../2-media-analysis-using-amazon-nova/01A-visual-segments-frames-shots-scenes.ipynb)\n",
    "2. Continue to [Additional Resources](../09-resources.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
