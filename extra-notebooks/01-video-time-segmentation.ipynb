{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6416fcdf-75c5-49fc-a9ec-adb8018df284",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Video time segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe197858",
   "metadata": {},
   "source": [
    "_Video time segmentation_ is an important data preparation step that can help unlock the full potential of video content for analysis and automation. By breaking down a video into meaningful segments, you can better understand the structure and context of the content, enabling a wide range of applications such as:\n",
    "\n",
    "* Identifying key events, scenes, or chapters within the video\n",
    "* Inserting metadata like ad markers or chapter markers\n",
    "* Reusing relevant clips or segments for new purposes\n",
    "* Applying advanced analytics and foundation models to specific parts of the video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b61422",
   "metadata": {},
   "source": [
    "![Video file decomposed into frames, shots and scenes](./static/images/01-visual-segments.png) \n",
    "\n",
    "***Figure:** Video file decomposed into frames, shots (numbered), and scenes (colored)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2545f85-1a23-4923-8577-f471f0293ce2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "In this notebook, you'll explore techniques to decompose a video into smaller segments using a combination of audio and visual cues. Specifically, you'll:\n",
    "\n",
    "* Break down the video into frames, shots, and scenes using visual analysis.\n",
    "* Extract the audio transcript and identify speech segments, sentences, and topics.\n",
    "* Combine the audio and visual segmentation to create a comprehensive, multi-modal understanding of the video structure.\n",
    "\n",
    "By the end of this notebook, you'll have a collection of segmentations for your video that can serve as a foundation for further analysis, automation, and reuse of the video assets. Let's get started!\n",
    "\n",
    "\n",
    "The outputs of this notebook will be used in the use case sections later on in this workshop.  For example, Ad Break Detection and Contextual Ad Placement, Video Summarization, and more.\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "üí° In an automated workflow, these audio and video segmentation tasks can be run in parallel since there is no dependency between the steps.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "üí° Video segmentation can be done along the temporal dimension or along the spatial dimension.  In the context of this notebook, the term ‚Äúsegmentation‚Äù will always be <i>time segmentation</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0182508-2fb3-4ad5-a154-02d94b92b493",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Key terms and definitions\n",
    "\n",
    "You can refer back to this section if you want the definition the terms used in the notebook.\n",
    "\n",
    "- **Frame** - frame image extracted from the video content\n",
    "- **Shot** - continuous sequences of frames between two edits or cuts that defines one action\n",
    "- **Scene** - continuous sequence of action taking place in a specific location and time, consisting of a series of shots.\n",
    "- **Transcript** - continuous sequence of words and punctuation representing the speech content of the audio track in a video.\n",
    "- **Sentence** - a full sentence in the English language.\n",
    "- **Subtitle** - a segment of text representing the speech in a video that is meant to be displayed for viewers as they watch the video.  \n",
    "- **Conversation topic (aka topic)** - A summarization of a group of sentences that contain discussion about a similar topic.\n",
    "- **Chapter** - logical divisions of the storyline of the video content, consisting of a series of shots and conversations on the similar topic\n",
    "- **WebVTT** - a file format used to store timed text track data, such as subtitles or captions, for video content on the web.\n",
    "- **Frame accurate timestamp** - a timestamp that can be mapped to a specific frame.  Frame accurate timestamps are useful for synchronization of video elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690876f8-42e1-43fa-9319-f0f6fda8e572",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Steps\n",
    "\n",
    "1. Prerequisites\n",
    "3. Extract frames from the video and create some basic frame metadata\n",
    "4. Detect shots\n",
    "5. Detect scenes\n",
    "6. Convert speech in the video to a text transcript and subtitles\n",
    "7. Detect sentences in the transcript\n",
    "8. Detect conversation topics\n",
    "9. Detect chapters in the video narrative using both visual and audio inputs\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "üí° Click on the list icon (<img src=\"video-understanding-with-generative-ai-on-aws-main/static/icons/list-solid.svg\" alt=\"list icon\" width=\"15\" height=\"15\">) in the left navigation panel in this Jupyter notebook to see the outline of the notebook and where you currently are.\n",
    "</div>\n",
    "\n",
    "Now, let's build!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed3f92-e7fd-4e2f-a8df-8646718e00ee",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488c3b7-2901-497e-8a1e-10d768f31943",
   "metadata": {},
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd909b-c112-408e-8b09-0c0e4de7ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453ab94-9b8a-42fe-a0e5-2b2e742d71fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import json\n",
    "import boto3\n",
    "import json_repair\n",
    "from termcolor import colored\n",
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "from IPython.display import Image as DisplayImage\n",
    "from lib.frames import VideoFrames\n",
    "from lib.shots import Shots\n",
    "from lib.scenes import Scenes\n",
    "from lib.transcript import Transcript\n",
    "from lib import util\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022f442-6ef9-4621-8c15-09eab7d82398",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Retrieve saved values from previous notebooks\n",
    "\n",
    "To run this notebook, you need to have run the previous notebook: [00_prerequisites.ipynb](./00-prequisites.ipynb), where you installed package dependencies and gathered some information from the SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ad005-d406-404e-9d6a-ef4d673980d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get variables from the previous notebook\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db107e-04e8-4604-9286-dac677a2eed8",
   "metadata": {},
   "source": [
    "### Download the sample video, Meridian, from Netflix\n",
    "\n",
    "The open source content is available under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7890d0-7859-4a59-853f-616da28b668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(url: str, output_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Download Netflix Meridian test video if not already present\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"Video already exists at {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    print(\"Downloading Netflix Meridian test video...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(output_path, 'wb') as file, tqdm(\n",
    "        desc=output_path,\n",
    "        total=total_size,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            size = file.write(data)\n",
    "            pbar.update(size)\n",
    "            \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26c3cfa-2e63-4611-83cb-98b932c16650",
   "metadata": {},
   "outputs": [],
   "source": [
    "video = {}\n",
    "video[\"path\"] = 'Netflix_Open_Content_Meridian.mp4'\n",
    "video[\"output_dir\"] = Path(video[\"path\"]).stem\n",
    "video[\"url\"] = f\"https://dx2y1cac29mt3.cloudfront.net/mp4/netflix/{video['path']}\"\n",
    "\n",
    "download_video(video[\"url\"], video[\"path\"])\n",
    "\n",
    "Video(url=video[\"url\"], width=640, height=360, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146cd6c8-c725-4832-815c-075df06e058e",
   "metadata": {},
   "source": [
    "# Visual segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57281a0b-9167-4e38-b3cc-d349bc93ac33",
   "metadata": {},
   "source": [
    "## Frame extraction \n",
    "\n",
    "We prepare visual elements by using similarity analysis of visual embeddings using Amazon Titan Multimodal Embedding (TME) model from Amazon Bedrock. First, we extract video frames at one frame per second with a 392√ó220 pixel resolution, optimized for visual quality and computational efficiency through numerous experiments. These extracted frames pass to the TME model to generate embeddings capturing visual features and semantics. The embeddings are compared to group visually similar frames into shots. Following are some shot examples:\n",
    "\n",
    "![Example of frames grouped into shots](./static/images/01-shot-examples.png)\n",
    "\n",
    "In this process, we sample one frame per second, then employ cosine similarity logic on adjacent frames to group frame images into shots, representing camera shot changes. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "üí° 392√ó220 pixel resolution is chosen to optimize the number of frames we can present to our chosen Foundation Model, Anthropic Claude Sonnet 3.5, while still retaining the level of detail we need for our use cases.  Different use cases may use higher or lower resolutions for lower cost or higher quality. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "üí° One frame per second sampling is a design choice that is suitable for the content used here, but can be adjusted for high-motion, high-frame-rate videos such as sports or more static video such as newsroom footage.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2c7fd1-0b85-43d8-b07c-b98224eb6215",
   "metadata": {},
   "source": [
    "### Extract frames from the video\n",
    "\n",
    "You'll be using a Python package, [_VideoFrames_](./lib/frames.py), to work with the video at the frame level.  This package is available on GitHub.  When you call the main method of VideoFrames, it will do the following steps to help prepare the video for frame based analysis with machine learning:\n",
    "\n",
    "1. Extract frames from the video, sampling at the specified frame rate, and store the resulting images in the folder `./<video name>/frames/`.\n",
    "2. The resulting frame metadata contains the following attributes for each frame:\n",
    "\n",
    "* **timestamp_millis** ‚Äî the timestamp, in milliseconds, where the frame appears in the video.  We'll use this timestamp to related video analysis results back to the video timeline.\n",
    "* **image_file** ‚Äî the location of the image in the `frames` folder.\n",
    "* **id** - the unique frame id\n",
    "\n",
    "Let's give it a try. \n",
    "\n",
    "‚è≥ Generating frames will take a few minutes to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271cbf6-00bc-464e-99e9-aed7366c22fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "video[\"frames\"] = VideoFrames(video[\"path\"], session['bucket'], max_res=(392, 220), sample_rate_fps=1, force=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3aa85a-02b6-45ac-ae82-792f99e43ec9",
   "metadata": {},
   "source": [
    "!ffmpeg -i Netflix_Open_Content_Meridian.mp4 -vf \"select='bitor(gte(t-prev_selected_t,1.000),isnan(prev_selected_t))',scale=392:220\" -vsync 0 -qmin 1 -q:v 1 -f image2 ./Netflix_Open_Content_Meridian/frames/frames%07d.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d67796b-a293-4876-978d-5bab46ab7ed6",
   "metadata": {},
   "source": [
    "!ffmpeg -i Netflix_Open_Content_Meridian.mp4 -vf \"select='if(eq(n,0),1,floor(t)-floor(prev_selected_t))',scale=392:220\" -vsync 0 -qmin 1 -q:v 1 -f image2 ./Netflix_Open_Content_Meridian/frames/frames%07d.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83fee98-3a79-4f2b-9d24-ddaa9d7d4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(JSON(video[\"frames\"].frames[0], root=\"first frame\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb110d-0f5b-430e-b87a-a4437caf7c0c",
   "metadata": {},
   "source": [
    "### Visualize the extracted frames\n",
    "\n",
    "Next, let's visualize the extracted frames of the video.  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "ü§î Do you notice any visual patterns in the frames? Based on the frames, can you predict how many shots are in the video?\n",
    "</div>\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400ef001-10c7-4b0a-8976-ba056dbf61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "video['frames'].display_frames(start=0, end=len(video['frames'].frames))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be884f2c-91a0-4fff-a922-82d385ebb6c7",
   "metadata": {},
   "source": [
    "## Shot Detection\n",
    "A shot is a continuous sequence of frames between two edits or cuts that define one action.  Usually, a shot represents a single camera position, but sometimes, shots may contain camera movements such as panning or zooming.  Based on this definition, frames that belong to the same shot should be similar.  Therefore, one way to implement shot detection would be to used image embeddings such as Amazon Titan Multimodal embeddings to group similar frames to shots.   However, since we are operating on sampled frames, the accuracy of timestamps for the shots would be limited by our frame sampling rate.\n",
    "\n",
    "For use cases like ad insertion, editing and search, the ideal is to use **frame accurate** timestamps that identify the exact frame where the shots begin and end.  [Amazon Rekognition's Segment API](https://docs.aws.amazon.com/rekognition/latest/dg/segments.html) is a video analysis service that automatically detects technical cues and shot boundaries in video content which provides frame-accurate timestamps for each shot boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3cd8c1-9186-4714-98ba-5e8e25893a03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frms = video['frames'].frames\n",
    "cosine_similarity(frms[0]['titan_multimodal_embedding'], frms[1]['titan_multimodal_embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504d3e32-127c-48dc-ad11-86a0c59135d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now, compare the second frame to the third frame.  The similarity score should be higher, since the main difference in these frames is the lettering with the words \"Los Angeles 1947\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd238d8-1691-44e8-a9aa-5e76a16565e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(frms[1]['titan_multimodal_embedding'], frms[2]['titan_multimodal_embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01f34c-f820-4575-a9c8-b18348358d87",
   "metadata": {},
   "source": [
    "Finally, compare the third frame to the fourth frame. Will is be lower or higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1299939-8219-4c08-bb96-4c16180cc755",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(frms[2]['titan_multimodal_embedding'], frms[3]['titan_multimodal_embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff52fcf-8ffe-432c-9f71-a26d60dcdd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video[\"shots\"] = Shots(video[\"frames\"], method=\"RekognitionShots\")\n",
    "\n",
    "print(f\"Number of shots: {len(video['shots'].shots)} from {len(video['frames'].frames)} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826c59c-2969-467b-ba9c-d342616ff242",
   "metadata": {},
   "source": [
    "Take a moment to look at the metadata for a sample shot from the results.  Each shot contains:\n",
    "\n",
    "* **method** - the method used to group frames into shots.  Possible values are `SimilarFrames` or `RekognitionShots`\n",
    "* **start_ms** - the starting timestamp of the shot\n",
    "* **end_ms** - the ending timestamp of the shot\n",
    "* **duration_ms** - the duration of the shot\n",
    "* **video_asset_dir** - the location of the metadata collected for this video\n",
    "* **start_frame_id** - the frame the shot begins with\n",
    "* **end_frame_id** - the frame the shot ends with\n",
    "* **composite_images** - a series of equal sized frame grids containing the frames for the shots.  Composite images can be used as inputs to multi-modal foundation models to generate insights about the shot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c372d-25d7-40bc-a4b3-c2e006323964",
   "metadata": {},
   "source": [
    "Show metadata for shot 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d653fdc-3c75-4c74-a927-944fb7f968e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(JSON(video[\"shots\"].shots[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc3760d-8ce2-465c-919c-c1842f7915a2",
   "metadata": {},
   "source": [
    "The `Shots` method creates a set of composite images consisting of the frames in each shot.  We will be using these composite images later on as inputs to Anthropic Claude Sonnet 3 on Amazon Bedrock to generate inferences to understand what is happening in the shots.  For now, you can examine the resulting composite images to visualize the shots. \n",
    "\n",
    "Show the composite images for shot 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6717b0eb-3ae9-42da-812b-174ab02a9e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shot = video[\"shots\"].shots[1]\n",
    "for idx, composite_image in enumerate(shot['composite_images']):\n",
    "    \n",
    "    print (f'\\nShot {shot[\"id\"] } Composite image file { idx+1 } of { len(shot[\"composite_images\"]) }: { composite_image[\"file\"] }\\n')\n",
    "    display(DisplayImage(filename=composite_image['file']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686b410-1e13-4c26-8745-f994ef5523b1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "You can also go back and adjust the SIMILARITY_THRESHOLD to experiment with how it changes the output.\n",
    "\n",
    "Before we move on to scene detection, let's take a look at all of the generated shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a1ede-670f-4d79-a271-1fe63737fe71",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize the shots\n",
    "for counter, shot in enumerate(video[\"shots\"].shots):\n",
    "    print(f'\\nSHOT {counter}: frames {shot[\"start_frame_id\"] } to {shot[\"end_frame_id\"] } =======\\n')\n",
    "\n",
    "    for image_file in shot['composite_images']:\n",
    "        display(DisplayImage(filename=image_file['file'], height=75))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc4856-45b3-4335-afb9-b0fbdefd2e8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "ü§î As you look through the lists of shots, do you notice any segments that could be improved?  If you do, you might try to identify some tricky situations for video segmentation.  These include:\n",
    "* Motion of the subject including rolling credits, cars, etc.\n",
    "* Motion of the camera in the form of panning shots and zooming shots\n",
    "* Fades and other transition effects\n",
    "\n",
    "There are more advanced techniques for handling these situations, but for our purposes, this result should work fine.\n",
    "  \n",
    "Finally, let's play a couple of adjacent shots and observe how they will look as a video clip.  Play `shot[12]`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d46686-f4f4-4f11-bbf3-bac5228bd0e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = video['shots'].shots[12]['start_ms']/1000\n",
    "end = video['shots'].shots[12]['end_ms']/1000\n",
    "shot_url = f'{video[\"url\"]}#t={start},{end}'\n",
    "Video(url=shot_url, width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf3c24-b9f7-4791-aba4-470cfb619382",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Play `shot[13]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac497fb1-6e91-4d10-bb68-e99563b9155b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = video['shots'].shots[13]['start_ms']/1000\n",
    "end = video['shots'].shots[13]['end_ms']/1000\n",
    "shot_url = f'{video[\"url\"]}#t={start},{end}'\n",
    "Video(url=shot_url, width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4dac4a-9954-4851-8e31-2f37b0c5e81c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "ü§î These shots show two people in the same room having a conversation that spans the shots.  Because these two shots are in the same setting, they belong together into a higher level grouping.  In the next section, we'll group shots together based on the visual information to get a more holistic view of the scene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a814895-5bbf-40dc-86e8-a1cec69a747f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Scene Detection\n",
    "\n",
    "\n",
    "Even after identifying individual camera shots, there may still be semantically similar shots depicting the same setting. To further cluster these into distinct scenes, we expand frame comparison beyond adjacent frames. By looking at similar frames across an expanded time window, we can identify shots that are likely part of the same contiguous scene. We calculate pairwise similarity scores between all frames within a given time window. Frames with similarity scores above a certain threshold are considered part of the same scene group. This process performs recursively across all frames in a shot. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "üí° The time window size and similarity threshold are parameters that can significantly impact the accuracy of scene boundary detection. In our example, a 30 second time window and 0.85 similarity threshold gave the best scene clustering results across our video samples, but this can be adjusted.\n",
    "</div>\n",
    "\n",
    "We accomplish scene grouping by first indexing all video frames using TME again and storing the embeddings along with their shot information and timestamps into a vector database, as illustrated in the following figure.\n",
    "\n",
    "![shots-to-scenes.png](static/images/01-shots-to-scenes.png)\n",
    "\n",
    "We then perform a recursive similarity search against this indexed frame corpus. For each frame, we find all other frames within a 3-minute time window in both directions with greater than 85% contextual similarity based on their vector representations. The shot information for these highly similar frames is recorded. This process iterates for all frames in a shot to compile contextually similar shots. This process repeats across all shots, and the compiled results look like this example:\n",
    "\n",
    "    shot 1 ‚Äì> 2, 3, 4\n",
    "    shot 2 ‚Äì> 1, 3\n",
    "    shot 3 ‚Äì> 2, 4, 5\n",
    "    shot 7 ‚Äì> 8, 9\n",
    "\n",
    "Finally, we run a reduction process to group shots that are mutually identified as highly similar into distinct scene groups as follows:\n",
    "\n",
    "    shot 1, 2, 3, 4, 5 ‚Üí scene 1\n",
    "    shot 7, 8, 9 ‚Üí scene 2\n",
    "\n",
    "This allows us to segment the initially detected shot boundaries into higher-level semantic scene boundaries based on visual and temporal coherence. The end-to-end process is illustrated in the following diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b9f11f-b9c7-44e5-b63b-ad87684e0646",
   "metadata": {},
   "source": [
    "### Create frame embeddings\n",
    "\n",
    "Image embeddings are numerical representations (vectors) of images that capture their essential features and characteristics.  These embeddings make it possible to perform mathematical operations on images and compare them in ways that align with human visual perception.  \n",
    "\n",
    "We'll create an image embedding for each frame using [Amazon Titan Multimodal Embeddings](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-multiemb-models.html) in Amazon Bedrock.  We'll be using helper functions in the [lib/frames.py](./lib/frames.py) to accomplish this task.  \n",
    "\n",
    "The calculated frame embeddings will be added to each frame in the `Frames` object that is stored in the `video` variable.\n",
    "\n",
    "Calling the `method make_titan_multimodal_embeddings()` from the `Frames` class will create frame embeddings and store them with the metadata for each frame.  \n",
    "\n",
    "‚è≥ Generating embeddings should take 2-5 minutes.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "If you get an <b>AccessDenied</b> error at this point, make sure you completed the step to enable model access for Amazon Titan Multimodal Embeddings and Anthropic Claude Sonnet 3 in the Amazon Bedrock console.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc64e12-f20b-4391-96da-af9580d78bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "video['frames'].make_titan_multimodal_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35a6727-2ea7-4a32-b05d-24821f88eae2",
   "metadata": {},
   "source": [
    "Use the next cell to print the metadata for the first frame and examine the `titan_multimodal_embedding` attribute.   It's a large vector that encodes the content of the frame in the vector space for the `amazon.titan-embed-image-v1` version of the Titan Multimodal Embeddings model.  When we compare this vector with other vectors encoded using the same model version, we can determine if they are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e7303e-6dc8-4809-a2f4-98ba416c6db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(JSON(video[\"frames\"].frames[0], root=\"first frame\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85819de-2430-4308-849d-044a25c0b104",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Next, lets do some comparisons of the first few frames using embeddings.  First, print a few of the sampled frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edae9046-0242-4dc2-bcf8-1752de7979f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video['frames'].display_frames(start=0, end=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3150cb-f2c3-45b5-a329-b92cd0471578",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In order to compare frames, we need a way to compare embeddings.  We'll implement a [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) function using the Python numpy package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d45abdf-096e-436b-9ec6-127d02329a91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    cos_sim = dot(a, b) / (norm(a) * norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5aeaf5-6262-484e-9e63-0bbf52d2bd2c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Next, let's test comparing some frames.\n",
    "\n",
    "Compare the first black frame to the second frame which is a view of a city street.  As expected, the similarity score is low as these frames are not very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d262bd-964c-4e1e-92db-7e91f68decd5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frms = video['frames'].frames\n",
    "cosine_similarity(frms[0]['titan_multimodal_embedding'], frms[1]['titan_multimodal_embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99c541c-09f2-4998-baf2-0e8c337511bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now, compare the second frame to the third frame.  The similarity score should be higher, since the main difference in these frames is the lettering with the words \"Los Angeles 1947\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac48082-b77e-483e-b33c-8bb83fc57c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(frms[1]['titan_multimodal_embedding'], frms[2]['titan_multimodal_embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bcd1e1-f73c-4007-bf89-c51629cdef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(frms[2]['titan_multimodal_embedding'], frms[3]['titan_multimodal_embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d367b3-8253-4fa6-a907-4ada3a6acd54",
   "metadata": {},
   "source": [
    "### Populate a FAISS vector store\n",
    "\n",
    "We will be using a local FAISS vector store so we can use a single search command to find all the frames that are similar to a particular frame all at once.  There are a number of different databases on AWS that can be used as a vector store.  One popular choice is Amazon Opensearch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b5b0e0-be79-429f-8535-78f7f5eb0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "video['frames'].make_vector_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f33d1-cab9-4af3-bb4e-b8610fc14c86",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Search for similar frames using the vector store\n",
    "\n",
    "First, let's use the vector store to find similar frames for the second frame in the video.  This is the first frame of the sequence that displays the words \"Los Angeles 1947\".  Based on inspection of the frames, we should get 3 _adjacent_ similar frames, but there are also several similar frames that are not adjacent.   \n",
    "\n",
    "Our similarity search function uses a [cosine similarity function](https://en.wikipedia.org/wiki/Cosine_similarity) to determine the [K nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm) in the vector space. There are two parameters that you can adjust to tune the result of the similarity search:\n",
    "\n",
    "* MIN_SIMILARITY is the similarity threshold.  \n",
    "* TIME_RANGE is the maximum time range to compare frames to from the input frame\n",
    "\n",
    "You can try different values of these parameters to get a feel for how the results change with tuning. The frames 0-19 are displayed after this to help visualize  the results.  Here are some good values to try:\n",
    "\n",
    "* MIN_SIMILARITY = .85, TIME_RANGE = 30\n",
    "* MIN_SIMILARITY = .80, TIME_RANGE = 30\n",
    "* MIN_SIMILARITY = .80, TIME_RANGE = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb87383f-4a7b-4614-93b2-d1494a300101",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MIN_SIMILARITY = .80\n",
    "TIME_RANGE = 30\n",
    "FRAME_ID = 1\n",
    "video['frames'].search_similarity(FRAME_ID, min_similarity = MIN_SIMILARITY, time_range = TIME_RANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c107959d-f991-4353-8158-986f59236e2b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video['frames'].display_frames(start=0, end=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859c9107-d4ce-4578-a3a3-1412902abe06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Detect Scenes\n",
    "\n",
    "Now let apply this similarity search to all the frames across shots to find similar shots.  If shots are similar within the TIME_RANGE, then they will be grouped to the same scene.\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Note: if you want to dive deeper into the scene detection code, you can find it in <a href=\"lib/scenes.py\">lib/scenes.py</a> in this project.  The objective for the workshop is to understand the general segmentation process and move quickly to prompt engineering and video understanding use cases.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780a526-5b4f-4d35-8143-0ed72041c388",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MIN_SIMILARITY = .80\n",
    "\n",
    "TIME_RANGE = 30\n",
    "\n",
    "video['scenes'] = Scenes(video['frames'], video['shots'].shots, MIN_SIMILARITY, TIME_RANGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706434f-e5f0-4668-a731-336bdbec7636",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Visualize the scenes\n",
    "\n",
    "Now let's visualize some scenes using the generated composite images. Note that some scenes will have more than one composite image.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "üí° Use the scroll bar in the output box to view the scenes.  Some scenes contain more frames than can fit on a single composite image, so the may be multiple composite images displayed for each scene.\n",
    "</div>\n",
    "\n",
    "ü§î Do you think the scenes grouped the shots in a way that makes sense?  Is there anything you would want to change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fae5b-f633-4b6f-9d11-c19e27b7f937",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize the scenes\n",
    "for counter, scene in enumerate(video[\"scenes\"].scenes):\n",
    "    print(f'\\nScene {counter}: frames {scene[\"start_frame_id\"] } to {scene[\"end_frame_id\"] } =======\\n')\n",
    "    video['frames'].display_frames(start=scene[\"start_frame_id\"], end=scene[\"end_frame_id\"]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ee898-13cc-47fc-90dc-8f69ba3db4a7",
   "metadata": {},
   "source": [
    "Finally, let's play a couple of adjacent scenes and observe how they will look as a video clips. As you play the video segments, pay attention to the transition of the _video and the audio_ from one scene to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206c3ae-4359-43d5-8745-5d7022aa2e3e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_scene = 10\n",
    "start = video['scenes'].scenes[start_scene]['start_ms']/1000\n",
    "end = video['scenes'].scenes[start_scene]['end_ms']/1000\n",
    "print(f\"scene { start_scene } duration: {video['scenes'].scenes[start_scene]['duration_ms']/1000} seconds\\n\")\n",
    "print(f\"start time: { start } end time: {end} seconds\\n\")\n",
    "shot_url = f'{video[\"url\"]}#t={start},{end}'\n",
    "Video(url=shot_url, width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105f194-39f4-425b-aaea-a86b14cfe801",
   "metadata": {},
   "source": [
    "Play the next scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5223b5-ccc1-4828-81f7-3e549a4f572c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_scene = start_scene + 1\n",
    "start = video['scenes'].scenes[start_scene]['start_ms']/1000\n",
    "end = video['scenes'].scenes[start_scene]['end_ms']/1000\n",
    "print(f\"scene { start_scene } duration: {video['scenes'].scenes[start_scene]['duration_ms']/1000} seconds\\n\")\n",
    "print(f\"start time: { start } end time: {end} seconds\\n\")\n",
    "shot_url = f'{video[\"url\"]}#t={start},{end}'\n",
    "Video(url=shot_url, width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab5a23-e97c-4522-9b80-c359fde05bdf",
   "metadata": {},
   "source": [
    "\n",
    "ü§î We just played scenes 9 and 10.  You may have noticed that the scene changed on a visual cue when the focus of the video turns towards the beach and the ocean.  _However_, this scene change occurs in the middle of the audio of the police dispatcher speaking over the radio.  If we want to identify clean breakpoints in the video to make clips, we should probably consider not only the visual content but the audio as well. That's exactly where we're headed in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ab3ae0-d099-470c-8630-207328ba7327",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Audio segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b31731-37cd-44ac-bedf-901103b280a7",
   "metadata": {},
   "source": [
    "### Generate a transcript, audio segments and WebVTT subtitles using Amazon Transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f75f1b-2ce4-4b9c-a870-b5bfd56ce4ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video['transcript'] = Transcript(video[\"path\"], session['bucket'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0353ce-7f0d-43a1-9681-de6f10c34a99",
   "metadata": {},
   "source": [
    "#### Examine the results from Amazon Transcribe\n",
    "The response from Amazon Transcribe contains a results dictionary with a transcript that contains a text-only transcript and a collection of items which contain each word and punctuation in the transcript along with a confidence score and timestamp for the item. The response also contains the same transcript formatted as subtitles in either WebVTT or SRT format. Let's take a look at these outputs.\n",
    "\n",
    "We will be using the WebVTT output for our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc0b01-a3e4-46a3-8385-d29d0672f1ca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Transcript JSON output\n",
    "\n",
    "The transcript `results` attribute contains several interesting and useful outputs:\n",
    "* **transcripts** - are a list of alternative text only transcripts for the video.  Our results is only configured to generate 1 alternative, but you can configure Amazon Transcribe to produce more, if needed.  Alternatives are just different semantic interpretations of the speech in the video.\n",
    "* **items** - items is a time series of `pronunciations` (aka words) and `puntuation` that Amazon Transcribe inferred from the speech in the video.  Because this is AI inference, there is a confidence score for each item. Finally, there is a start and end time for each item, which can use to align timing of the items with other timestamped elements of the video.\n",
    "* **audio_segments** - audio segments contains a list of sentences or phrases Amazon Transcribe inferred from the Speech in the video. We'll refer to audio segments interchangeably as sentences within this workshop.  Because this is natural language, speech to text doesn't always contain proper sentence, thus the more general term used by Amazon Transcribe.\n",
    "\n",
    "Take a moment to examine each of these attributes from the sample video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a331b9e-cc86-4ed8-a8de-bf03829b73dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(filename=video['transcript'].transcript_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd334b64-011e-4405-ae18-86b505650ffc",
   "metadata": {},
   "source": [
    "**WebVTT Subtitles**\n",
    "\n",
    "Let's take a look at the first few lines of the WebVTT formatted subtitles generated by Amazon Transcribe.  WebVTT subtitles can be used in video players to display the speech in the video as text on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36dafd9-7520-46da-93aa-0f0f71d2cb73",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head {video['transcript'].vtt_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb94b5d-906f-4028-9907-df6a38377fc1",
   "metadata": {},
   "source": [
    "Finally, let's view the video with the generated subtitle track.  Note: we used the shot information to start the video at the shot where the first speech occurs in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5071a3-b3ab-463b-bbc6-27489497242a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Play the video with subtitles\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "start = video['shots'].shots[8]['start_ms']/1000\n",
    "end = video['shots'].shots[8]['end_ms']/1000\n",
    "speech_shot_url = f'{video[\"url\"]}#t={start}'\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "    <source src=\"{speech_shot_url}\" type=\"video/mp4\">\n",
    "    <track src=\"{video['transcript'].vtt_file}\" kind=\"captions\" srclang=\"en\" label=\"English\" default>\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "HTML(video_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aa3ec9-995f-4912-9dab-f33d0b5e0959",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "So far, you have used fairly basic building blocks such as the structural information in the video from FFmpeg,  similarity of frames, and English language syntax from speech to text processing, to create time based segments of video.  In the next section, you will finally start to perform semantic analysis, or _video understanding_, to create segments using the ideas expressed in the video content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf388e0-f71d-4bff-bc4b-f6b44659bcbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Generate conversation topics using Amazon Bedrock\n",
    "\n",
    "In this next section, you will use generative AI to understand the conversation topics that are occurring over time in the video transcript.  This is a text summarization task that can be performed by several Foundation Models.  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "üí° We'll be using Anthropic Claude Sonnet 3 on Amazon Bedrock for this and all the other generative AI tasks throughout this workshop.  We chose Anthropic Claude Sonnet 3 because of its flexibility to perform a variety of tasks on multimodal (image and text) inputs.  In practice, you may want to substitute different FMs for different tasks based on your requirements.  For example, you may find that Anthropic Claude Haiku produces adequate results at a lower price points for a specific use case.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e602f7-c9b8-4c9d-97c2-444efe83e243",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You will pass the transcript to the Anthropic Claude 3 Sonnet model on Amazon Bedrock. The model analyzes the transcript and suggests conversational topic points in a specific JSON format. In the prompt, you specify that each topic should contain a start and end timestamp along with a reason describing the topic. The prompts for the Sonnet model are shown below.  Note that this prompt uses the [Anthropic Claude Messages API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html):\n",
    "\n",
    "**System prompt**\n",
    "\n",
    "```\n",
    "You are a media operation assistant who analyses movie transcripts in WebVTT \n",
    "format and suggest topic points based on the topic changes in the conversations. \n",
    "It is important to read the entire transcripts.\n",
    "```\n",
    "\n",
    "\n",
    "**Messages**\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        'content': 'Here is the transcripts in <transcript> tag:\\n'\n",
    "                '<transcript>{transcript}\\n</transcript>\\n',\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'content': 'OK. I got the transcript. What output format?',\n",
    "        'role': 'assistant'\n",
    "    },\n",
    "    {\n",
    "        'content': 'JSON format. An example of the output:\\n'\n",
    "                '{\"topics\": [{\"start\": \"00:00:10.000\", \"end\": \"00:00:32.000\", '\n",
    "                '\"reason\": \"It appears the topic talks about...\"}]}\\n',\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'content': '{', 'role': 'assistant'\n",
    "    }\n",
    " ]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039f33b-2e09-46dc-8cba-182d0deacbc8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    ‚ùì <b>Ask Amazon Q Developer</b>: What are the inputs for the Anthropic Claude Messages API?\n",
    "    <br></br>* You may have noticed the Amazon Q Developer menu icon in the side bar menu of this Jupyter notebook.  Anytime you have a question about the code in the notebook or about AWS APIs, try asking Amazon Q.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b91c4f-6b57-44ca-886f-1811771c0da4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### The code below constructs the prompt for Amazon Bedrock and then calls the Amazon Bedrock API to execute the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7f4e7-ead4-40ac-a22f-9a30a61790c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lib import bedrock_helper as brh\n",
    "from lib import util\n",
    "\n",
    "MODEL_ID = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "MODEL_VER = 'bedrock-2023-05-31'\n",
    "CLAUDE_PRICING = (0.00025, 0.00125)\n",
    "\n",
    "def analyze_conversations(vtt_file):\n",
    "\n",
    "    response = {}\n",
    "    messages = []\n",
    "\n",
    "    # transcript\n",
    "    transcript_message = make_transcript(vtt_file)\n",
    "    messages.append(transcript_message)\n",
    "\n",
    "    # output format?\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'Got the transcript. What output format?'\n",
    "    })\n",
    "\n",
    "    # example output\n",
    "    example_message = make_conversation_example()\n",
    "    messages.append(example_message)\n",
    "\n",
    "    # prefill output\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': '{'\n",
    "    })\n",
    "\n",
    "    ## system prompt to role play\n",
    "    system = 'You are a media operation assistant who analyses movie transcripts in WebVTT format and suggest topic points based on the topic changes in the conversations. It is important to read the entire transcripts.'\n",
    "\n",
    "    ## setting up the model params\n",
    "    model_params = {\n",
    "        'anthropic_version': MODEL_VER,\n",
    "        'max_tokens': 4096,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.7,\n",
    "        'top_k': 20,\n",
    "        'stop_sequences': ['\\n\\nHuman:'],\n",
    "        'system': system,\n",
    "        'messages': messages\n",
    "    }\n",
    "\n",
    "    response['model_params'] = model_params\n",
    "    try:\n",
    "        response['response'] = brh.inference(model_params)\n",
    "    except Exception as e:\n",
    "        print(colored(f\"ERR: inference: {str(e)}\\n RETRY...\", 'red'))\n",
    "        response['response'] = brh.inference(model_params)\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "def make_conversation_example():\n",
    "    example = {\n",
    "        'topics': [\n",
    "            {\n",
    "                'start': '00:00:10.000',\n",
    "                'end': '00:00:32.000',\n",
    "                'reason': 'It appears the topic talks about...'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': 'JSON format. An example of the output:\\n{0}\\n'.format(json.dumps(example))\n",
    "    }\n",
    "\n",
    "def make_transcript(vtt_file):\n",
    "    with open(vtt_file, encoding=\"utf-8\") as f:\n",
    "        transcript = f.read()\n",
    "    \n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': 'Here is the transcripts in <transcript> tag:\\n<transcript>{0}\\n</transcript>\\n'.format(transcript)\n",
    "    }\n",
    "\n",
    "def make_conversation_message(text):\n",
    "    message = {\n",
    "        'role': 'user',\n",
    "        'content': 'No conversation.'\n",
    "    }\n",
    "\n",
    "    if text:\n",
    "        message['content'] = 'Here is the conversation of the scene in <conversation> tag.\\n<conversation>\\n{0}\\n</conversation>\\n'.format(text)\n",
    "\n",
    "    return message\n",
    "\n",
    "def chapters_to_vtt(chapters, output_file):\n",
    "    \"\"\"\n",
    "      Constructs a webvtt caption file based on the timestamps from the given chapters.\n",
    "      Args:\n",
    "         chapters - the topic points\n",
    "         output_file - output file where the caption webvtt content is stored.\n",
    "      Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    vtt_lines = 'WEBVTT\\n\\n'\n",
    "    for idx, chapter in enumerate(chapters):\n",
    "        line = f\"{idx}\\n{chapter['start']} --> {chapter['end']}\\n{chapter['reason']}\\n\"\n",
    "        vtt_lines = vtt_lines+line\n",
    "\n",
    "    util.save_to_file(output_file, vtt_lines)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287629d3-233e-4ecb-b0b7-1a5a920bff67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Execute the prompt and examine the output\n",
    "\n",
    "The model will return a result formatted the way that was specified using sample output in the prompt `messages`.  In this case it is a list of topics in the video.  Each topic has:\n",
    "\n",
    "* **start** - the start time of the topic relative to the start of the video in HH:MM:SS.MS format\n",
    "* **end** - the start time of the topic relative to the start of the video in HH:MM:SS.MS format\n",
    "* **reason** - the summary of the conversation in the time range between `start` and `end`\n",
    "\n",
    "Run the next cells to execute the prompt and look at the resulting topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d7864-1a65-45ca-bf6e-d126a41a1f80",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversations_response = analyze_conversations(video['transcript'].vtt_file)\n",
    "video['topics'] = conversations_response['response']['content'][0]['json']['topics']\n",
    "\n",
    "# show the conversation cost\n",
    "# conversation_cost = brh.display_conversation_cost(conversations_response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a2c34-9ae7-4e49-b86c-4d3866bfe2da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(JSON(video['topics'], root='topics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb3380e-44e7-47fc-adca-f715aec1f801",
   "metadata": {},
   "source": [
    "#### Finally, let's take one last look at the actual prompt passed to Anthropic Claude with all the parameters filled in.\n",
    "\n",
    "The `system` prompt outlines the task and constraints for the Anthropic Claude, while the `messages` part of the prompt, model a conversation with the FM.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f8d81-26ad-447f-a536-106e4472fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(conversations_response['model_params'], root='prompt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec2e96-7be9-44f8-8ddb-1e5e704bd696",
   "metadata": {},
   "source": [
    "### Generate \"topic points\" \n",
    "\n",
    "To ensure the model's output accurately reflects the original transcript, the output JSON is post-processed to merge any overlapping chapter timestamps and align the chapter boundaries with the actual caption timestamps from the WebVTT file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f994abb0-7b62-428c-9767-49a81a9cf83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b0dfe-0604-4841-a1fb-719a4bfea83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import topics\n",
    "## parse the conversation\n",
    "#conversations = video['topics']\n",
    "\n",
    "## merge overlapped conversation timestamps\n",
    "video['topics'] = topics.merge_topics(video['topics'])\n",
    "\n",
    "## validate the conversation timestamps against the caption timestamps\n",
    "captions = topics.parse_webvtt(video['transcript'].vtt_file)\n",
    "video['topics'] = topics.validate_timestamps(video['topics'], captions)\n",
    "\n",
    "## save the conversations\n",
    "util.save_to_file(os.path.join('./Netflix_Open_Content_Meridian', 'topics.json'), video['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e575ce-4108-4102-a809-0b7870d2f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(video['topics'], root='topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8ab79-9d0f-4cf9-8893-dadecb07f23a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Visualize the topics\n",
    "Finally, visualize the topics with the video.  We'll output the chapter summaries as a WebVTT file and play them with the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80c20b-ba02-4e7f-b0b2-042514dbd4bc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video['topics_vtt'] = os.path.join(video['frames'].video_asset_dir(), \"topics.vtt\")\n",
    "chapters_to_vtt(video['topics'], video['topics_vtt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500f720-7c1f-4801-ae1f-fa0e439da80a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Play the video with topic summaries\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "start = video['shots'].shots[8]['start_ms']/1000\n",
    "end = video['shots'].shots[8]['end_ms']/1000\n",
    "speech_shot_url = f'{video[\"url\"]}#t={start}'\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "    <source src=\"{speech_shot_url}\" type=\"video/mp4\">\n",
    "    <track src=\"{video['topics_vtt']}\" kind=\"captions\" srclang=\"en\" label=\"English\" default>\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "HTML(video_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba889078-83ae-4bce-a1d9-af198caef450",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You just used generative AI to make topic summaries of a video.  Neat trick!\n",
    "\n",
    "ü§î Can you think of other use cases for summarizing different time segments of audio or video content?\n",
    "<br></br>\n",
    "ü§î How would you increase the number of topics found by the Foundation Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a997c-26a6-4dad-b888-6d1908934cc1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98de755-2cd6-451a-9570-7b26474447e7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Store the video metadata so it can be used in the rest of the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6a12e-9e70-4120-b1de-1b23f827c2a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768c9f6-c3a4-4539-afd7-0056d5604d97",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Continue to the next section of the workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa2234-0461-4488-a50b-dbfd8c8ea48a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In the next sections of the workshop, you will use Geneative AI on Amazon Bedrock to generate contextual metadata about the video segments you created in this part of the workshop.  From here you can proceed to any use case notebook.  \n",
    "\n",
    "For the in-person workshop, go to the next notebook [Ad break detection and contextual Ad tartgeting](02-ad-breaks-and-contextual-ad-targeting.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239a20aa-e48f-4541-9179-08443602462b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### References for further learning\n",
    "\n",
    "* [Media2Cloud Solution V4 Solution Guidance on GitHub](https://github.com/aws-solutions-library-samples/guidance-for-media2cloud-on-aws)\n",
    "* [Rekognition Video Segmentation API code sample](https://github.com/aws-samples/amazon-rekognition-code-samples/blob/main/rekognition-apis/6-video-segment-detection.ipynb)\n",
    "* [Amazon Bedrock Workshop](https://github.com/aws-samples/amazon-bedrock-workshop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0088f39e-19ab-4edc-9b78-7d8e654340cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
