{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6416fcdf-75c5-49fc-a9ec-adb8018df284",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Audio segments using speech to text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe197858",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "The audio part of a video provides numerous clues about the structure and narrative of a video. Audio contains:\n",
    "\n",
    "\n",
    "\n",
    "* Speech, which can be further broken down into different language segments such as words, punctuation, sentences or subtitles\n",
    "* Sounds such as crowd noise at a sporting event, foley effects or background music\n",
    "* Sound levels including silences\n",
    "\n",
    "Figure 1, below, shows a clip of Meridian with the audio waveform that corresponds the sound levels at different points in time in the video.\n",
    "\n",
    "![Video with audio waveform](./static/images/01-meridian-audio-wide.png)\n",
    "**Figure 1. Video with audio waveform**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2545f85-1a23-4923-8577-f471f0293ce2",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "In this notebook, you'll continue to explore techniques to decompose a video into segments using audio, focusing on the speech in the video. Specifically, you'll:\n",
    "\n",
    "* Use Amazon Transcribe to extract the audio transcript and identify words, sentences, subtitles, and speech segments.\n",
    "* Use a Foundation Model to identify segments of video that contain different conversation topics.\n",
    "* Combine conversation topics with visual scenes to identify chapters in the video.\n",
    "\n",
    "The outputs of this notebook will be used in the use case sections later on in this workshop.  For example, Ad Break Detection and Contextual Ad Placement, Video Summarization, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0182508-2fb3-4ad5-a154-02d94b92b493",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Key terms and definitions\n",
    "\n",
    "You can refer back to this section if you want the definition the terms used in the notebook.\n",
    "\n",
    "- **Transcript** - continuous sequence of words and punctuation representing the speech content of the audio track in a video.\n",
    "- **Subtitle** - a segment of text representing the speech in a video that is meant to be displayed for viewers as they watch the video.\n",
    "- **Speech Segement** - speech segements are an output of Amazon Transcribe that  \n",
    "- **Conversation topic (aka topic)** - A summarization of a group of sentences that contain discussion about a similar topic.\n",
    "- **WebVTT** - a file format used to store timed text track data, such as subtitles or captions, for video content on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54483bf9-04f4-44c6-92b9-9bf2d7047882",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Workflow\n",
    "\n",
    "The purpose of this lab is to give hands-on practice working with the audio elements of video using speech to text and to practice prompt engineering with timed text derived from audio.  You will execute AWS service APIs from SageMaker Studio.  The workflow takes a video as input and produces a transcript, sentence segments, WebVTT format subtitles, and conversation topics outputs.\n",
    "\n",
    "![Audio Workflow](./static/images/01-audio-workflow.png)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "üí° Click on the list icon in the left navigation panel in this Jupyter notebook to see the outline of the notebook and where you currently are.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed3f92-e7fd-4e2f-a8df-8646718e00ee",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488c3b7-2901-497e-8a1e-10d768f31943",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453ab94-9b8a-42fe-a0e5-2b2e742d71fb",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import json_repair\n",
    "from termcolor import colored\n",
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "from IPython.display import Image as DisplayImage\n",
    "from lib.frames import VideoFrames\n",
    "from lib.shots import Shots\n",
    "from lib.scenes import Scenes\n",
    "from lib.transcript import Transcript\n",
    "from lib import util\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022f442-6ef9-4621-8c15-09eab7d82398",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Retrieve saved values from previous notebooks\n",
    "\n",
    "To run this notebook, you need to have run the previous notebook: [01A-visual-segments-frames-shots-scenes.ipynb](01A-visual-segments-frames-shots-scenes.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501ad005-d406-404e-9d6a-ef4d673980d6",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get variables from the previous notebook\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa577b6f-b260-42a1-be85-ed71b9cc91f3",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b31731-37cd-44ac-bedf-901103b280a7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Generate a transcript, audio segments and WebVTT subtitles using Amazon Transcribe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2244ca-bc6c-4af3-bb69-3dd4f78fbfcd",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Transcript library\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note</b>: The code for Transcript library is located in the <b>lib</b> folder of this project if you want to dive deeper, but the objective of this exercise is to understand the concept of a transcript.    \n",
    "</div>\n",
    "\n",
    "Click the link to open [lib/transcript.py](lib/transcript.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05138c47-4707-44ac-88ee-8cc98e5a8e7d",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Generate the transcript and other derived outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f75f1b-2ce4-4b9c-a870-b5bfd56ce4ff",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video['transcript'] = Transcript(video[\"path\"], session['bucket'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0353ce-7f0d-43a1-9681-de6f10c34a99",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Examine the results from Amazon Transcribe\n",
    "The response from Amazon Transcribe contains a results dictionary with a transcript that contains a text-only transcript and a collection of items which contain each word and punctuation in the transcript along with a confidence score and timestamp for the item. The response also contains the same transcript formatted as subtitles in either WebVTT or SRT format. Let's take a look at these outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbc0b01-a3e4-46a3-8385-d29d0672f1ca",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "##### Transcript JSON output\n",
    "\n",
    "The transcript `results` attribute contains several interesting and useful outputs:\n",
    "* **transcripts** - are a list of alternative text only transcripts for the video.  Our results is only configured to generate 1 alternative, but you can configure Amazon Transcribe to produce more, if needed.  Alternatives are just different semantic interpretations of the speech in the video.\n",
    "* **items** - items is a time series of `pronunciations` (aka words) and `punctuation` that Amazon Transcribe inferred from the speech in the video.  Because this is AI inference, there is a _confidence score_ for each item. In Amazon Transcribe, confidence scores represent how confident the service is in the accuracy of each transcribed word.  Finally, there is a start and end time for each item, which can use to align timing of the items with other timestamped elements of the video.\n",
    "* **audio_segments** - audio segments contains a list of distinct speech segments detected by Amazon Transcribe.  Segment boundaries are determined by:\n",
    "    * Natural speech pauses\n",
    "    * Speaker changes\n",
    "    * Maximum segment duration limits\n",
    "    * Punctuation\n",
    "\n",
    "Amazon Transcribe also outputs \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    ‚ùì <b>Ask Amazon Q Developer</b>: What are the attributed of items returned from Amazon Transcribe?\n",
    "    ‚ùì <b>Ask Amazon Q Developer</b>: What are confidence scores in Amazon Transcribe?\n",
    "</div>\n",
    "\n",
    "Take a moment to examine each of these attributes from the sample video below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a331b9e-cc86-4ed8-a8de-bf03829b73dc",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(filename=video['transcript'].transcript_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd334b64-011e-4405-ae18-86b505650ffc",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### WebVTT Subtitles\n",
    "\n",
    "Let's take a look at the first few lines of the WebVTT formatted subtitles generated by Amazon Transcribe.  WebVTT subtitles can be used in video players to display the speech in the video as text on the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36dafd9-7520-46da-93aa-0f0f71d2cb73",
   "metadata": {
    "editable": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo \"$(<{video['transcript'].vtt_file})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88471866-4743-4eef-8d20-2f22f0b42be3",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Play subtitles with the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb94b5d-906f-4028-9907-df6a38377fc1",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Finally, let's view the video with the generated subtitle track.  Note: we used the shot information to start the video at the shot where the first speech occurs in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5071a3-b3ab-463b-bbc6-27489497242a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Play the video with subtitles\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "start = video['shots'].shots[8]['start_ms']/1000\n",
    "end = video['shots'].shots[8]['end_ms']/1000\n",
    "speech_shot_url = f'{video[\"url\"]}#t={start}'\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "    <source src=\"{speech_shot_url}\" type=\"video/mp4\">\n",
    "    <track src=\"{video['transcript'].vtt_file}\" kind=\"captions\" srclang=\"en\" label=\"English\" default>\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "HTML(video_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf388e0-f71d-4bff-bc4b-f6b44659bcbe",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Generate conversation topics using Amazon Bedrock\n",
    "\n",
    "In this next section, you will use generative AI to understand the conversation topics that are occurring over time in the video transcript.  This is a text summarization task that can be performed by several Foundation Models.  \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "üí° We'll be using Anthropic Claude Sonnet 3 on Amazon Bedrock for this and all the other generative AI tasks throughout this workshop.  We chose Anthropic Claude Sonnet 3 because of its flexibility to perform a variety of tasks on multimodal (image and text) inputs.  In practice, you may want to substitute different FMs for different tasks based on your requirements.  For example, you may find that Anthropic Claude Haiku produces adequate results at a lower price points for a specific use case.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e602f7-c9b8-4c9d-97c2-444efe83e243",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You will pass the transcript to the Anthropic Claude 3 Sonnet model on Amazon Bedrock. The model analyzes the transcript and suggests conversational topic points in a specific JSON format. In the prompt, you specify that each topic should contain a start and end timestamp along with a reason describing the topic. The prompts for the Sonnet model are shown below.  Note that this prompt uses the [Anthropic Claude Messages API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html):\n",
    "\n",
    "**System prompt**\n",
    "\n",
    "```\n",
    "You are a media operation assistant who analyses movie transcripts in WebVTT \n",
    "format and suggest topic points based on the topic changes in the conversations. \n",
    "It is important to read the entire transcripts.\n",
    "```\n",
    "\n",
    "\n",
    "**Messages**\n",
    "\n",
    "```\n",
    "[\n",
    "    {\n",
    "        'content': 'Here is the transcripts in <transcript> tag:\\n'\n",
    "                '<transcript>{transcript}\\n</transcript>\\n',\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'content': 'OK. I got the transcript. What output format?',\n",
    "        'role': 'assistant'\n",
    "    },\n",
    "    {\n",
    "        'content': 'JSON format. An example of the output:\\n'\n",
    "                '{\"topics\": [{\"start\": \"00:00:10.000\", \"end\": \"00:00:32.000\", '\n",
    "                '\"reason\": \"It appears the topic talks about...\"}]}\\n',\n",
    "        'role': 'user'\n",
    "    },\n",
    "    {\n",
    "        'content': '{', 'role': 'assistant'\n",
    "    }\n",
    " ]\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5039f33b-2e09-46dc-8cba-182d0deacbc8",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    ‚ùì <b>Ask Amazon Q Developer</b>: What are the inputs for the Anthropic Claude Messages API?\n",
    "    <br></br>* You may have noticed the Amazon Q Developer menu icon in the side bar menu of this Jupyter notebook.  Anytime you have a question about the code in the notebook or about AWS APIs, try asking Amazon Q.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b91c4f-6b57-44ca-886f-1811771c0da4",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### The code below constructs the prompt for Amazon Bedrock and then calls the Amazon Bedrock API to execute the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba7f4e7-ead4-40ac-a22f-9a30a61790c8",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lib import bedrock_helper as brh\n",
    "from lib import util\n",
    "\n",
    "MODEL_ID = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "MODEL_VER = 'bedrock-2023-05-31'\n",
    "CLAUDE_PRICING = (0.00025, 0.00125)\n",
    "\n",
    "def analyze_conversations(vtt_file):\n",
    "\n",
    "    response = {}\n",
    "    messages = []\n",
    "\n",
    "    # transcript\n",
    "    transcript_message = make_transcript(vtt_file)\n",
    "    messages.append(transcript_message)\n",
    "\n",
    "    # output format?\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'Got the transcript. What output format?'\n",
    "    })\n",
    "\n",
    "    # example output\n",
    "    example_message = make_conversation_example()\n",
    "    messages.append(example_message)\n",
    "\n",
    "    # prefill output\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': '{'\n",
    "    })\n",
    "\n",
    "    ## system prompt to role play\n",
    "    system = '''\n",
    "    You are a media operation assistant who analyses movie transcripts in WebVTT format\n",
    "    and suggest topic points based on the topic changes in the conversations. \n",
    "    It is important to read the entire transcript.\n",
    "    '''\n",
    "\n",
    "    ## setting up the model params\n",
    "    model_params = {\n",
    "        'anthropic_version': MODEL_VER,\n",
    "        'max_tokens': 4096,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.7,\n",
    "        'top_k': 20,\n",
    "        'stop_sequences': ['\\n\\nHuman:'],\n",
    "        'system': system,\n",
    "        'messages': messages\n",
    "    }\n",
    "\n",
    "    response['model_params'] = model_params\n",
    "    try:\n",
    "        response['response'] = inference(model_params)\n",
    "    except Exception as e:\n",
    "        print(colored(f\"ERR: inference: {str(e)}\\n RETRY...\", 'red'))\n",
    "        response['response'] = inference(model_params)\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "def make_conversation_example():\n",
    "    example = {\n",
    "        'topics': [\n",
    "            {\n",
    "                'start': '00:00:10.000',\n",
    "                'end': '00:00:32.000',\n",
    "                'reason': 'It appears the topic talks about...'\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': 'JSON format. An example of the output:\\n{0}\\n'.format(json.dumps(example))\n",
    "    }\n",
    "\n",
    "def make_transcript(vtt_file):\n",
    "    with open(vtt_file, encoding=\"utf-8\") as f:\n",
    "        transcript = f.read()\n",
    "    \n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': 'Here is the transcripts in <transcript> tag:\\n<transcript>{0}\\n</transcript>\\n'.format(transcript)\n",
    "    }\n",
    "\n",
    "def make_conversation_message(text):\n",
    "    message = {\n",
    "        'role': 'user',\n",
    "        'content': 'No conversation.'\n",
    "    }\n",
    "\n",
    "    if text:\n",
    "        message['content'] = 'Here is the conversation of the scene in <conversation> tag.\\n<conversation>\\n{0}\\n</conversation>\\n'.format(text)\n",
    "\n",
    "    return message\n",
    "\n",
    "def chapters_to_vtt(chapters, output_file):\n",
    "    \"\"\"\n",
    "      Constructs a webvtt caption file based on the timestamps from the given chapters.\n",
    "      Args:\n",
    "         chapters - the topic points\n",
    "         output_file - output file where the caption webvtt content is stored.\n",
    "      Returns:\n",
    "         None\n",
    "    \"\"\"\n",
    "    vtt_lines = 'WEBVTT\\n\\n'\n",
    "    for idx, chapter in enumerate(chapters):\n",
    "        line = f\"{idx}\\n{chapter['start']} --> {chapter['end']}\\n{chapter['reason']}\\n\"\n",
    "        vtt_lines = vtt_lines+line\n",
    "\n",
    "    util.save_to_file(output_file, vtt_lines)\n",
    "    return\n",
    "\n",
    "def inference(model_params):\n",
    "    model_id = MODEL_ID\n",
    "    accept = 'application/json'\n",
    "    content_type = 'application/json'\n",
    "\n",
    "    bedrock_runtime_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    response = bedrock_runtime_client.invoke_model(\n",
    "        body=json.dumps(model_params),\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type\n",
    "    )\n",
    "\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    # patch the json string output with '{' and parse it\n",
    "    response_content = response_body['content'][0]['text']\n",
    "    if response_content[0] != '{':\n",
    "        response_content = '{' + response_content\n",
    "\n",
    "    try:\n",
    "        response_content = json.loads(response_content)\n",
    "    except Exception as e:\n",
    "        print(colored(\"Malformed JSON response. Try to repair it...\", 'red'))\n",
    "        try:\n",
    "            response_content = json_repair.loads(response_content, strict=False)\n",
    "        except Exception as e:\n",
    "            print(colored(\"Failed to repair the JSON response...\", 'red'))\n",
    "            print(colored(response_content, 'red'))\n",
    "            raise e\n",
    "\n",
    "    response_body['content'][0]['json'] = response_content\n",
    "    response_body['model_params'] = model_params\n",
    "\n",
    "    return response_body\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287629d3-233e-4ecb-b0b7-1a5a920bff67",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Execute the prompt and examine the output\n",
    "\n",
    "The model will return a result formatted the way that was specified using sample output in the prompt `messages`.  In this case it is a list of topics in the video.  Each topic has:\n",
    "\n",
    "* **start** - the start time of the topic relative to the start of the video in HH:MM:SS.MS format\n",
    "* **end** - the start time of the topic relative to the start of the video in HH:MM:SS.MS format\n",
    "* **reason** - the summary of the conversation in the time range between `start` and `end`\n",
    "\n",
    "Run the next cells to execute the prompt and look at the resulting topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7d7864-1a65-45ca-bf6e-d126a41a1f80",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conversations_response = analyze_conversations(video['transcript'].vtt_file)\n",
    "video['topics'] = conversations_response['response']['content'][0]['json']['topics']\n",
    "\n",
    "# show the conversation cost\n",
    "conversation_cost = brh.display_conversation_cost(conversations_response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a2c34-9ae7-4e49-b86c-4d3866bfe2da",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(JSON(video['topics'], root='topics'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb3380e-44e7-47fc-adca-f715aec1f801",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Finally, let's take one last look at the actual prompt passed to Anthropic Claude with all the parameters filled in.\n",
    "\n",
    "The `system` prompt outlines the task and constraints for the Anthropic Claude, while the `messages` part of the prompt, model a conversation with the FM.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f8d81-26ad-447f-a536-106e4472fbf3",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(conversations_response['model_params'], root='prompt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec2e96-7be9-44f8-8ddb-1e5e704bd696",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Generate \"topic points\" \n",
    "\n",
    "To ensure the model's output accurately reflects the original transcript, the output JSON is post-processed to merge any overlapping chapter timestamps and align the chapter boundaries with the actual caption timestamps from the WebVTT file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b0dfe-0604-4841-a1fb-719a4bfea83b",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from lib import topics\n",
    "\n",
    "# merge overlapped conversation timestamps\n",
    "video['topics'] = topics.merge_topics(video['topics'])\n",
    "\n",
    "# validate the conversation timestamps against the caption timestamps\n",
    "captions = topics.parse_webvtt(video['transcript'].vtt_file)\n",
    "video['topics'] = topics.validate_timestamps(video['topics'], captions)\n",
    "\n",
    "# save the conversations\n",
    "util.save_to_file(os.path.join(video[\"output_dir\"], 'topics.json'), video['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e575ce-4108-4102-a809-0b7870d2f784",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(video['topics'], root='topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8ab79-9d0f-4cf9-8893-dadecb07f23a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Visualize the topics\n",
    "Finally, visualize the topics with the video.  We'll output the chapter summaries as a WebVTT file and play them with the video.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Note: the topics are distributed across the entire 12 minute video.  You can skip through the video by clicking on different points on the player timeline rather than playing the whole video.  See if you can find all 5 topics.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80c20b-ba02-4e7f-b0b2-042514dbd4bc",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video['topics_vtt'] = os.path.join(video['frames'].video_asset_dir(), \"topics.vtt\")\n",
    "chapters_to_vtt(video['topics'], video['topics_vtt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500f720-7c1f-4801-ae1f-fa0e439da80a",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Play the video with topic summaries\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "start = video['shots'].shots[8]['start_ms']/1000\n",
    "end = video['shots'].shots[8]['end_ms']/1000\n",
    "speech_shot_url = f'{video[\"url\"]}#t={start}'\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "    <source src=\"{speech_shot_url}\" type=\"video/mp4\">\n",
    "    <track src=\"{video['topics_vtt']}\" kind=\"captions\" srclang=\"en\" label=\"English\" default>\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "HTML(video_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba889078-83ae-4bce-a1d9-af198caef450",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You just used generative AI to make topic summaries of a video.  Neat trick!\n",
    "\n",
    "ü§î Can you think of other use cases for summarizing different time segments of audio or video content?\n",
    "<br></br>\n",
    "ü§î How would you increase the number of topics found by the Foundation Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98de755-2cd6-451a-9570-7b26474447e7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Store the video metadata so it can be used in the rest of the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee6a12e-9e70-4120-b1de-1b23f827c2a7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5768c9f6-c3a4-4539-afd7-0056d5604d97",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa2234-0461-4488-a50b-dbfd8c8ea48a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now that you have created some basic segmentations and prompts against video clips, you are ready to apply these techniques to solve some use cases.  From here, you can choose a use case to explore further.\n",
    "\n",
    "* [Ad break detection and contextual Ad tartgeting](02-ad-breaks-and-contextual-ad-targeting.ipynb) - identify opportunities for ad insertion.  Use a standard taxonomy to match video content to ad content.\n",
    "* [Video summarization](03-video-summarization.ipynb) - generate short form videos from a longer video\n",
    "* [Semantic video search](04-semantic-video-search.ipynb) - search video using images and natural language to find relavent clips\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eede4a-597e-40cd-ab80-b5e31a03f669",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
